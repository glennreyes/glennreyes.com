
export default [
  {
    "content": "2019 was full of exciting things. Since I haven't done a review like this before nor have I written any blog posts (duh!), I wanted to take the opportunity and sum up the most important things I have done this year.\n\n## Traveling\n\n<Image\n  src=\"/images/2019/traveling.jpg\"\n  width={2234}\n  height={3972}\n  alt=\"Traveling\"\n/>\n\nI visited 8 different countries:\n\n- üá∫üá∏ USA\n- üá≤üáΩ Mexico\n- üáµüá≠ Philippines\n- üá™üá∏ Spain\n- üá©üá™ Germany\n- üáµüá± Poland\n- üá≠üá∑ Croatia\n- üá∫üá¶ Ukraine\n\n<Image\n  src=\"/images/2019/sunset.jpg\"\n  width={3024}\n  height={4032}\n  alt=\"Sunset in Philippines\"\n/>\n\nWhere as the ones overseas were mostly for leisure and the closer ones for business. Vacation was mostly spent for quality time reasons with the family and relatives. Besides, I got to do some fun things like...\n\n<Image\n  src=\"/images/2019/courtside.jpg\"\n  width={3024}\n  height={4032}\n  alt=\"Courtside NBA\"\n/>\n\n... Watching an (my first!) NBA game from a court side seat in OKC or...\n\n<Image\n  src=\"/images/2019/diving.jpg\"\n  width={5471}\n  height={3613}\n  alt=\"Diving in Bohol, Philippines\"\n/>\n\n... Getting a PADI Open Water Scuba Diving license.\n\n<Image\n  src=\"/images/2019/warsaw.png\"\n  width={2214}\n  height={2767}\n  alt=\"Warsaw metropole\"\n/>\n\nDuring my travels to Spain, Germany, Poland, Croatia and Ukraine I attended to dozens of developer conferences to share my experiences about React & GraphQL.\n\n## Giving workshops about GraphQL\n\nThis year I started something that I always wanted to do: Teaching. I gave workshops to help people working with GraphQL. Back in April, [React Finland](https://react-finland.fi) gave me the opportunity to host my first workshop. From there I was able to continue giving workshops at other events and places. Here's the full list:\n\n- üá´üáÆ [React Finland](https://react-finland.fi)\n- üá™üá∏ [React Alicante](https://reactalicante.es)\n- üá¶üáπ [UpLeveled](https://upleveled.io)\n- üá≠üá∑ [WebCamp Zagreb](https://2019.webcampzg.org)\n- üáµüá± [FrontEnd Con](https://frontend-con.io)\n\n<Image\n  src=\"/images/2019/workshop.jpg\"\n  width={3088}\n  height={2320}\n  alt=\"Workshop in Alicante, Spain\"\n/>\n\nGiving workshops is something which I'm totally passionate about which I'm definitely looking forward to continuing in the upcoming years. So keep updated and check out my [workshops](/workshops).\n\n## Public speaking\n\n<Image\n  src=\"/images/2019/speaking.jpg\"\n  width={1250}\n  height={2000}\n  alt=\"Speaking at JSFest Autumn\"\n/>\n\nThis year I was able to share my thoughts and experiences at 11 conferences and events. I met super nice people and friends along the way. I'm super grateful for all event organizers for giving me the opportunity. I'm looking forward for further collabs in the future.\n\n<Image\n  src=\"/images/2019/speaking.jpg\"\n  width={3024}\n  height={4032}\n  alt=\"Speaking at WebCamp Zagreb\"\n/>\n\n## Focusing health\n\nIn December 2018, I was diagnosed with a [lipid disorder](https://en.wikipedia.org/wiki/Hyperlipidemia) so I decided to take serious actions in 2019.\n\nBasically, I was recommended to doing this:\n\n- Eat skinless poultry with no visible fat\n- Eat lean meats, in moderate portions\n- Eat low-fat or fat-free dairy products\n- Consume polyunsaturated fats and mono-unsaturated fat instead of saturated fats and trans fats\n- Exercise for at least 30 minutes per day, 4 days per week\n- Avoid fast food, junk food, and processed meats\n- Eating grilled and roasted foods instead of fried foods\n- Drink less alcohol (triglyceride)\n\n> _tl;dr I had to lose some body weight and stay in shape._\n\n### Pescatarian diet\n\nI decided to switch to a pescatarian diet which allowed me to eat healthier while consuming valuable Omega-3 fatty acids to lower my [triglycerides](https://www.mayoclinic.org/diseases-conditions/high-blood-cholesterol/in-depth/triglycerides/art-20048186) and [LDL levels](https://en.wikipedia.org/wiki/Low-density_lipoprotein).\n\n<Image\n  src=\"/images/2019/food.jpg\"\n  width={4032}\n  height={3024}\n  alt=\"Shaka bowls in Bohol, Philippines\"\n/>\n\nPescatarian is fairly easy to follow and I got used to it in just a few months. Cutting meat was for sure not an easy task but overall, I feel noticably better having them replaced by vegetables and fruits.\n\nIn the future I will definitely continue pescatarian and focus more towards plantbased products.\n\n### Sports\n\nI started to dedicate around 3-4 hours per week in actively doing sports. In the first months it was mostly running and strength workouts. Now, I'm trying to go regularly for a swim, ride and run at least once each per week.\n\n<Image\n  src=\"/images/2019/cycling.jpg\"\n  width={4032}\n  height={3024}\n  alt=\"Cycling in Vienna\"\n/>\n\nI track all my [activities on Strava](https://www.strava.com/athletes/14875783), feel free to follow along.\n\n### Losing body weight\n\nI dropped 14kg body weight in one year. Ideally, I lose another 6kg and keep at ideal BMI range. It's worth mentioning that I care about healthy nutrition and activities more than losing weight. But it's nice to have it as another metric for measuring purposes.\n\n## What to expect in 2020\n\nIn the upcoming year I will help two companies build products in React and React Native.\n\n### Teaching and speaking\n\nAs of today, I have only confirmed to speak at [AgentConf 2020](https://agent.sh) in January. In 2020, I will be reducing submitting talks to conference so I can focus more on actual work and things I'm also passionate about.\n\n<Image\n  src=\"/images/2019/speaking-2.jpg\"\n  width={2000}\n  height={1250}\n  alt=\"Speaking at JSFest Autumn\"\n/>\n\nI will be continuing giving workshops and do company trainings about GraphQL & React. So if you're a conference/community/event organizer or intested in an on-site company training, feel free to [reach out](mailto:glenn@glennreyes.com) and check out [my workshops](/workshops).\n\n### Creating music\n\nI used to do music and play guitar often during the years when I was studying and have completely neglected it since I started to work. I want to change that. I will dedicate some time in creating music and relearning the basics.\n\n### Health\n\nAs in 2019, I will try to stay pescatarian but focus more on consuming plantbased products. I will also do my best to keep exercising regularly.",
    "description": "2019 was thrilling! I'm summing up my top accomplishments this year in this review/blog post.",
    "publishedAt": "2019-12-31",
    "title": "2019: Year in Review",
    "_meta": {
      "filePath": "2019.mdx",
      "fileName": "2019.mdx",
      "directory": ".",
      "extension": "mdx",
      "path": "2019"
    },
    "body": "2019 was full of exciting things. Since I haven't done a review like this before nor have I written any blog posts (duh!), I wanted to take the opportunity and sum up the most important things I have done this year.\n\n## Traveling\n\n<Image\n  src=\"/images/2019/traveling.jpg\"\n  width={2234}\n  height={3972}\n  alt=\"Traveling\"\n/>\n\nI visited 8 different countries:\n\n- üá∫üá∏ USA\n- üá≤üáΩ Mexico\n- üáµüá≠ Philippines\n- üá™üá∏ Spain\n- üá©üá™ Germany\n- üáµüá± Poland\n- üá≠üá∑ Croatia\n- üá∫üá¶ Ukraine\n\n<Image\n  src=\"/images/2019/sunset.jpg\"\n  width={3024}\n  height={4032}\n  alt=\"Sunset in Philippines\"\n/>\n\nWhere as the ones overseas were mostly for leisure and the closer ones for business. Vacation was mostly spent for quality time reasons with the family and relatives. Besides, I got to do some fun things like...\n\n<Image\n  src=\"/images/2019/courtside.jpg\"\n  width={3024}\n  height={4032}\n  alt=\"Courtside NBA\"\n/>\n\n... Watching an (my first!) NBA game from a court side seat in OKC or...\n\n<Image\n  src=\"/images/2019/diving.jpg\"\n  width={5471}\n  height={3613}\n  alt=\"Diving in Bohol, Philippines\"\n/>\n\n... Getting a PADI Open Water Scuba Diving license.\n\n<Image\n  src=\"/images/2019/warsaw.png\"\n  width={2214}\n  height={2767}\n  alt=\"Warsaw metropole\"\n/>\n\nDuring my travels to Spain, Germany, Poland, Croatia and Ukraine I attended to dozens of developer conferences to share my experiences about React & GraphQL.\n\n## Giving workshops about GraphQL\n\nThis year I started something that I always wanted to do: Teaching. I gave workshops to help people working with GraphQL. Back in April, [React Finland](https://react-finland.fi) gave me the opportunity to host my first workshop. From there I was able to continue giving workshops at other events and places. Here's the full list:\n\n- üá´üáÆ [React Finland](https://react-finland.fi)\n- üá™üá∏ [React Alicante](https://reactalicante.es)\n- üá¶üáπ [UpLeveled](https://upleveled.io)\n- üá≠üá∑ [WebCamp Zagreb](https://2019.webcampzg.org)\n- üáµüá± [FrontEnd Con](https://frontend-con.io)\n\n<Image\n  src=\"/images/2019/workshop.jpg\"\n  width={3088}\n  height={2320}\n  alt=\"Workshop in Alicante, Spain\"\n/>\n\nGiving workshops is something which I'm totally passionate about which I'm definitely looking forward to continuing in the upcoming years. So keep updated and check out my [workshops](/workshops).\n\n## Public speaking\n\n<Image\n  src=\"/images/2019/speaking.jpg\"\n  width={1250}\n  height={2000}\n  alt=\"Speaking at JSFest Autumn\"\n/>\n\nThis year I was able to share my thoughts and experiences at 11 conferences and events. I met super nice people and friends along the way. I'm super grateful for all event organizers for giving me the opportunity. I'm looking forward for further collabs in the future.\n\n<Image\n  src=\"/images/2019/speaking.jpg\"\n  width={3024}\n  height={4032}\n  alt=\"Speaking at WebCamp Zagreb\"\n/>\n\n## Focusing health\n\nIn December 2018, I was diagnosed with a [lipid disorder](https://en.wikipedia.org/wiki/Hyperlipidemia) so I decided to take serious actions in 2019.\n\nBasically, I was recommended to doing this:\n\n- Eat skinless poultry with no visible fat\n- Eat lean meats, in moderate portions\n- Eat low-fat or fat-free dairy products\n- Consume polyunsaturated fats and mono-unsaturated fat instead of saturated fats and trans fats\n- Exercise for at least 30 minutes per day, 4 days per week\n- Avoid fast food, junk food, and processed meats\n- Eating grilled and roasted foods instead of fried foods\n- Drink less alcohol (triglyceride)\n\n> _tl;dr I had to lose some body weight and stay in shape._\n\n### Pescatarian diet\n\nI decided to switch to a pescatarian diet which allowed me to eat healthier while consuming valuable Omega-3 fatty acids to lower my [triglycerides](https://www.mayoclinic.org/diseases-conditions/high-blood-cholesterol/in-depth/triglycerides/art-20048186) and [LDL levels](https://en.wikipedia.org/wiki/Low-density_lipoprotein).\n\n<Image\n  src=\"/images/2019/food.jpg\"\n  width={4032}\n  height={3024}\n  alt=\"Shaka bowls in Bohol, Philippines\"\n/>\n\nPescatarian is fairly easy to follow and I got used to it in just a few months. Cutting meat was for sure not an easy task but overall, I feel noticably better having them replaced by vegetables and fruits.\n\nIn the future I will definitely continue pescatarian and focus more towards plantbased products.\n\n### Sports\n\nI started to dedicate around 3-4 hours per week in actively doing sports. In the first months it was mostly running and strength workouts. Now, I'm trying to go regularly for a swim, ride and run at least once each per week.\n\n<Image\n  src=\"/images/2019/cycling.jpg\"\n  width={4032}\n  height={3024}\n  alt=\"Cycling in Vienna\"\n/>\n\nI track all my [activities on Strava](https://www.strava.com/athletes/14875783), feel free to follow along.\n\n### Losing body weight\n\nI dropped 14kg body weight in one year. Ideally, I lose another 6kg and keep at ideal BMI range. It's worth mentioning that I care about healthy nutrition and activities more than losing weight. But it's nice to have it as another metric for measuring purposes.\n\n## What to expect in 2020\n\nIn the upcoming year I will help two companies build products in React and React Native.\n\n### Teaching and speaking\n\nAs of today, I have only confirmed to speak at [AgentConf 2020](https://agent.sh) in January. In 2020, I will be reducing submitting talks to conference so I can focus more on actual work and things I'm also passionate about.\n\n<Image\n  src=\"/images/2019/speaking-2.jpg\"\n  width={2000}\n  height={1250}\n  alt=\"Speaking at JSFest Autumn\"\n/>\n\nI will be continuing giving workshops and do company trainings about GraphQL & React. So if you're a conference/community/event organizer or intested in an on-site company training, feel free to [reach out](mailto:glenn@glennreyes.com) and check out [my workshops](/workshops).\n\n### Creating music\n\nI used to do music and play guitar often during the years when I was studying and have completely neglected it since I started to work. I want to change that. I will dedicate some time in creating music and relearning the basics.\n\n### Health\n\nAs in 2019, I will try to stay pescatarian but focus more on consuming plantbased products. I will also do my best to keep exercising regularly.",
    "slug": "2019"
  },
  {
    "content": "<Image\n  src=\"/opengraph-image.png\"\n  width={1920}\n  height={1080}\n  alt=\"Glenn Reyes\"\n/>\n\nAfter using Gatsby for my personal site for half a decade, I felt it was time to rebuild it from scratch. Gatsby offered several features I appreciated, such as the GraphQL data layer, easy page creation, and static site generation.\n\nHowever, I also faced some challenges, including outdated content, a broken deployment process, suboptimal data structure for appearances, talks, and workshops, and the desire for a fresh look and feel with updated technologies. Thus, it was the perfect time to start from the ground up.\n\n## Framework\n\n<Image\n  src=\"/images/2023-rebuild/remix-vs-next.png\"\n  width={1740}\n  height={1300}\n  alt=\"Remix vs Next.js\"\n/>\n\nAs I began the rebuilding process, I considered two frameworks: [Remix](https://remix.run) and [Next.js](https://nextjs.org). After experimenting with both, I chose Next.js 13 for my personal website for several reasons:\n\n- I wanted to learn more about React Server Components (RSC)\n- I was intrigued by the new [Next.js App Router](https://beta.nextjs.org/docs/routing/fundamentals)\n- Next.js, in combination with Vercel, allowed for quick and easy site setup and deployment\n\nI plan to explore Remix in future projects, as I appreciate its focus on the Web API as a first-class citizen.\n\n## React Server Components\n\nMy interest in learning about React Server Components was a key factor in choosing Next.js. While experimenting with both frameworks, I attempted to build the early stages of my site without relying on documentation to see how far I could progress. However, I quickly realized I needed to dive deeper into RSC. Some insights I gained include:\n\n- **Optimizing my React components organization for working with RSC**: As I incorporated RSC, I became more mindful of when and where to create React components with hooks and interactivity.\n- **Creating server-rendered components with selective client rehydration.**\n- **Safely using API secrets in RSC without exposing them to the client.**\n\nAlthough RSC is still in its early stages, I am excited about having built my entire website using this technology. I am eager to see how it evolves and gains support from various libraries.\n\n## Data\n\nFor my personal website, I sought a data layer that was easy to work with and that I could envision using for the next five years. I explored several options, eventually settling on the following.\n\n### MDX\n\nSeveral options compatible with the Next.js App Router were available, such as the [built-in support for MDX](https://beta.nextjs.org/docs/guides/mdx#mdx) in Next.js and [`next-mdx-remote`](https://github.com/hashicorp/next-mdx-remote#react-server-components-rsc--nextjs-app-directory-support). I primarily use `next-mdx-remote` for blog content and static pages.\n\n### Database\n\nI use a database to manage content for [talks](/talks) and [workshops](/workshops), providing the flexibility to design and scale my data model. The database layer pairs well with React's `cache` API for deduping and caching data responses from queries.\n\n## Design\n\nPreviously, I used Figma for designing my site before starting the development process. However, this time I chose to design directly in the browser with Tailwind CSS, which allowed for quick and easy experimentation with various design elements, such as fonts, layouts, and colors.\n\n<Image\n  src=\"/images/2023-rebuild/design.png\"\n  width={1536}\n  height={3648}\n  alt=\"Design\"\n/>\n\nI drew inspiration from several projects, including:\n\n- [Spotlight by Tailwind CSS](https://tailwindui.com/templates/spotlight)\n- [Lee Robinson's personal website](https://leerob.io)\n- [Delba de Oliveira's personal website](https://delba.dev)\n- [Brian Lovin's personal website](https://brianlovin.com)\n\nI enjoyed tinkering and crafting visually appealing user interfaces and often invest a considerable amount of time refining their appearance.\n\n<Image\n  src=\"/images/2023-rebuild/themes.png\"\n  width={1545}\n  height={1017}\n  alt=\"Themes\"\n/>\n\n## Moving forward\n\nI am delighted with the outcome of my new site and eagerly anticipate adding more content and features. Additionally, I look forward to deepening my understanding of React Server Components and leveraging their potential to enhance my site's user experience.\n\nThe source code of my site is available on [GitHub](https://github.com/glennreyes/glennreyes.com) and I love to hear your thoughts on the project!",
    "description": "My journey of rebuilding my personal website from scratch using Next.js, discussing design inspirations, data management, and my experience working with React Server Components.",
    "publishedAt": "2023-04-14",
    "title": "Rebuilding My Personal Site with Next.js and RSC",
    "_meta": {
      "filePath": "2023-rebuild.mdx",
      "fileName": "2023-rebuild.mdx",
      "directory": ".",
      "extension": "mdx",
      "path": "2023-rebuild"
    },
    "body": "<Image\n  src=\"/opengraph-image.png\"\n  width={1920}\n  height={1080}\n  alt=\"Glenn Reyes\"\n/>\n\nAfter using Gatsby for my personal site for half a decade, I felt it was time to rebuild it from scratch. Gatsby offered several features I appreciated, such as the GraphQL data layer, easy page creation, and static site generation.\n\nHowever, I also faced some challenges, including outdated content, a broken deployment process, suboptimal data structure for appearances, talks, and workshops, and the desire for a fresh look and feel with updated technologies. Thus, it was the perfect time to start from the ground up.\n\n## Framework\n\n<Image\n  src=\"/images/2023-rebuild/remix-vs-next.png\"\n  width={1740}\n  height={1300}\n  alt=\"Remix vs Next.js\"\n/>\n\nAs I began the rebuilding process, I considered two frameworks: [Remix](https://remix.run) and [Next.js](https://nextjs.org). After experimenting with both, I chose Next.js 13 for my personal website for several reasons:\n\n- I wanted to learn more about React Server Components (RSC)\n- I was intrigued by the new [Next.js App Router](https://beta.nextjs.org/docs/routing/fundamentals)\n- Next.js, in combination with Vercel, allowed for quick and easy site setup and deployment\n\nI plan to explore Remix in future projects, as I appreciate its focus on the Web API as a first-class citizen.\n\n## React Server Components\n\nMy interest in learning about React Server Components was a key factor in choosing Next.js. While experimenting with both frameworks, I attempted to build the early stages of my site without relying on documentation to see how far I could progress. However, I quickly realized I needed to dive deeper into RSC. Some insights I gained include:\n\n- **Optimizing my React components organization for working with RSC**: As I incorporated RSC, I became more mindful of when and where to create React components with hooks and interactivity.\n- **Creating server-rendered components with selective client rehydration.**\n- **Safely using API secrets in RSC without exposing them to the client.**\n\nAlthough RSC is still in its early stages, I am excited about having built my entire website using this technology. I am eager to see how it evolves and gains support from various libraries.\n\n## Data\n\nFor my personal website, I sought a data layer that was easy to work with and that I could envision using for the next five years. I explored several options, eventually settling on the following.\n\n### MDX\n\nSeveral options compatible with the Next.js App Router were available, such as the [built-in support for MDX](https://beta.nextjs.org/docs/guides/mdx#mdx) in Next.js and [`next-mdx-remote`](https://github.com/hashicorp/next-mdx-remote#react-server-components-rsc--nextjs-app-directory-support). I primarily use `next-mdx-remote` for blog content and static pages.\n\n### Database\n\nI use a database to manage content for [talks](/talks) and [workshops](/workshops), providing the flexibility to design and scale my data model. The database layer pairs well with React's `cache` API for deduping and caching data responses from queries.\n\n## Design\n\nPreviously, I used Figma for designing my site before starting the development process. However, this time I chose to design directly in the browser with Tailwind CSS, which allowed for quick and easy experimentation with various design elements, such as fonts, layouts, and colors.\n\n<Image\n  src=\"/images/2023-rebuild/design.png\"\n  width={1536}\n  height={3648}\n  alt=\"Design\"\n/>\n\nI drew inspiration from several projects, including:\n\n- [Spotlight by Tailwind CSS](https://tailwindui.com/templates/spotlight)\n- [Lee Robinson's personal website](https://leerob.io)\n- [Delba de Oliveira's personal website](https://delba.dev)\n- [Brian Lovin's personal website](https://brianlovin.com)\n\nI enjoyed tinkering and crafting visually appealing user interfaces and often invest a considerable amount of time refining their appearance.\n\n<Image\n  src=\"/images/2023-rebuild/themes.png\"\n  width={1545}\n  height={1017}\n  alt=\"Themes\"\n/>\n\n## Moving forward\n\nI am delighted with the outcome of my new site and eagerly anticipate adding more content and features. Additionally, I look forward to deepening my understanding of React Server Components and leveraging their potential to enhance my site's user experience.\n\nThe source code of my site is available on [GitHub](https://github.com/glennreyes/glennreyes.com) and I love to hear your thoughts on the project!",
    "slug": "2023-rebuild"
  },
  {
    "content": "Do you use it? No?\n\nWell, it‚Äôs time to look at it. Not next week, now.\n\n<Image\n  src=\"/images/ai-agents/robot.png\"\n  width={1920}\n  height={1080}\n  alt=\"Developer communicating with a robot\"\n/>\n\nIf you write code for a living, you‚Äôve probably already seen the wave of AI powered tools for your code editor take over your timeline:\n\n- GitHub Copilot\n- Cursor\n- Claude Code\n- GPT-5-Codex\n- Windsurf\n\nCall them AI coding agents or editor-native assistants. Whatever the name, they‚Äôre becoming impossible to ignore.\n\nThese are no longer just autocomplete tools. They‚Äôre turning into full-blown agents inside your editor, able to refactor a file, explain an error, scaffold a new feature, or even coordinate across your entire codebase.\n\n<Video>\n  <source src=\"/images/ai-agents/cursor-agent.mp4\" type=\"video/mp4\" />A video\n  demonstrating the cursor AI agent\n</Video>\n\n## What They Actually Are\n\nThink of them as companions embedded directly in your workflow. They live inside editors like VS Code or Cursor, can look across related files for context, and are able to plan and execute tasks intelligently.\n\nThey‚Äôre not replacing you. They‚Äôre becoming the ultimate pair programmer. At times they even feel like a junior dev who happens to outperform you in the boring parts.\n\n---\n\n## Who They‚Äôre For\n\nAI coding agents help in different ways depending on who you are. Juniors get on-demand mentorship and fast scaffolding. Seniors move quicker on boilerplate, refactors, and tests. Teams get consistency and unblock each other faster. Indies and startups gain the speed they need without building everything by hand.\n\nBasically: if you write code in 2025, they‚Äôre for you.\n\n---\n\n## The Downsides\n\nThere are trade-offs. Pro plans aren‚Äôt cheap, often priced like adding another SaaS engineer to your payroll. Even with bigger context windows you‚Äôll still run into ‚Äútoo long‚Äù errors on large codebases. Hallucinations remain real, so output needs review. And of course every vendor wants to lock you into their ecosystem.\n\nNot perfect, but not deal-breakers either.\n\n<Image\n  src=\"/images/ai-agents/robot-heart.png\"\n  width={1920}\n  height={1080}\n  alt=\"Developer and robot coding together\"\n/>\n\n## Why They‚Äôre Worth It\n\nThe upside is massive. You can move from idea to prototype in hours. You learn faster, with inline explanations of unfamiliar APIs. You keep consistency when agents enforce patterns, lint rules, or your design system. And because the cost of failure drops, you‚Äôll explore more ideas.\n\nMost of all, they reduce mental overhead. Things that used to feel heavy (setting up CI, writing integration tests, restructuring files) can be delegated to the agent. That frees you up for architecture, design, and product thinking.\n\n<Image\n  src=\"/images/ai-agents/sunset.png\"\n  width={1920}\n  height={1080}\n  alt=\"Sunset\"\n/>\n\n## The Impact on Developers\n\nAI coding agents don‚Äôt make developers obsolete. They shift the focus. Instead of typing boilerplate, you think about systems. Instead of wrestling with errors, you design better experiences. Instead of slow iteration, you prototype at speed.\n\nEvery day, sometimes every hour, new updates and features drop. It‚Äôs impossible to keep up with all of it. But one thing is clear: AI coding agents give you superpowers.\n\nThe developers who embrace them will feel like they‚Äôre coding with jetpacks. Those who ignore them will start to feel slow.\n\n---\n\nüëâ Bottom line: don‚Äôt sleep on AI coding agents. They‚Äôre not hype anymore. They‚Äôre already reshaping what it means to be a software engineer in 2025.",
    "description": "AI coding agents are changing how developers work. This post explains what they are, their pros and cons, and why they give you superpowers.",
    "publishedAt": "2025-10-06",
    "title": "Don‚Äôt Sleep on AI Coding Agents",
    "_meta": {
      "filePath": "ai-agents.mdx",
      "fileName": "ai-agents.mdx",
      "directory": ".",
      "extension": "mdx",
      "path": "ai-agents"
    },
    "body": "Do you use it? No?\n\nWell, it‚Äôs time to look at it. Not next week, now.\n\n<Image\n  src=\"/images/ai-agents/robot.png\"\n  width={1920}\n  height={1080}\n  alt=\"Developer communicating with a robot\"\n/>\n\nIf you write code for a living, you‚Äôve probably already seen the wave of AI powered tools for your code editor take over your timeline:\n\n- GitHub Copilot\n- Cursor\n- Claude Code\n- GPT-5-Codex\n- Windsurf\n\nCall them AI coding agents or editor-native assistants. Whatever the name, they‚Äôre becoming impossible to ignore.\n\nThese are no longer just autocomplete tools. They‚Äôre turning into full-blown agents inside your editor, able to refactor a file, explain an error, scaffold a new feature, or even coordinate across your entire codebase.\n\n<Video>\n  <source src=\"/images/ai-agents/cursor-agent.mp4\" type=\"video/mp4\" />A video\n  demonstrating the cursor AI agent\n</Video>\n\n## What They Actually Are\n\nThink of them as companions embedded directly in your workflow. They live inside editors like VS Code or Cursor, can look across related files for context, and are able to plan and execute tasks intelligently.\n\nThey‚Äôre not replacing you. They‚Äôre becoming the ultimate pair programmer. At times they even feel like a junior dev who happens to outperform you in the boring parts.\n\n---\n\n## Who They‚Äôre For\n\nAI coding agents help in different ways depending on who you are. Juniors get on-demand mentorship and fast scaffolding. Seniors move quicker on boilerplate, refactors, and tests. Teams get consistency and unblock each other faster. Indies and startups gain the speed they need without building everything by hand.\n\nBasically: if you write code in 2025, they‚Äôre for you.\n\n---\n\n## The Downsides\n\nThere are trade-offs. Pro plans aren‚Äôt cheap, often priced like adding another SaaS engineer to your payroll. Even with bigger context windows you‚Äôll still run into ‚Äútoo long‚Äù errors on large codebases. Hallucinations remain real, so output needs review. And of course every vendor wants to lock you into their ecosystem.\n\nNot perfect, but not deal-breakers either.\n\n<Image\n  src=\"/images/ai-agents/robot-heart.png\"\n  width={1920}\n  height={1080}\n  alt=\"Developer and robot coding together\"\n/>\n\n## Why They‚Äôre Worth It\n\nThe upside is massive. You can move from idea to prototype in hours. You learn faster, with inline explanations of unfamiliar APIs. You keep consistency when agents enforce patterns, lint rules, or your design system. And because the cost of failure drops, you‚Äôll explore more ideas.\n\nMost of all, they reduce mental overhead. Things that used to feel heavy (setting up CI, writing integration tests, restructuring files) can be delegated to the agent. That frees you up for architecture, design, and product thinking.\n\n<Image\n  src=\"/images/ai-agents/sunset.png\"\n  width={1920}\n  height={1080}\n  alt=\"Sunset\"\n/>\n\n## The Impact on Developers\n\nAI coding agents don‚Äôt make developers obsolete. They shift the focus. Instead of typing boilerplate, you think about systems. Instead of wrestling with errors, you design better experiences. Instead of slow iteration, you prototype at speed.\n\nEvery day, sometimes every hour, new updates and features drop. It‚Äôs impossible to keep up with all of it. But one thing is clear: AI coding agents give you superpowers.\n\nThe developers who embrace them will feel like they‚Äôre coding with jetpacks. Those who ignore them will start to feel slow.\n\n---\n\nüëâ Bottom line: don‚Äôt sleep on AI coding agents. They‚Äôre not hype anymore. They‚Äôre already reshaping what it means to be a software engineer in 2025.",
    "slug": "ai-agents"
  },
  {
    "content": "Here's an uncomfortable question I've been sitting with lately.\n\nWe're all racing to build AI-powered web apps. Chatbots, document processors, code assistants, productivity tools. The market is exploding, and everyone wants a piece. But what if we're building on the wrong layer?\n\n## The Shift Happening Right Now\n\nMCP (Model Context Protocol) is changing the game. Instead of users opening your web app, logging in, and using your interface, they can now connect directly to AI agents that access your functionality through MCP servers.\n\nThink about what that means:\n\n- No more context switching between apps\n- No more copying and pasting between tools\n- No more \"let me open that other tab\"\n\nThe AI agent becomes the interface. Your carefully designed UI? Your onboarding flow? Your sticky features that keep users coming back? They might all become optional.\n\nWe've already seen glimpses of this with ChatGPT Apps and custom GPTs. Users don't visit separate websites anymore. They just talk to their AI agent, and it handles the rest.\n\n> The best interface is no interface by [Golden Krishna](https://x.com/goldenkrishna)\n\n## A Pattern We've Seen Before\n\nRemember when every business needed a mobile app? Newspapers gave way to radio. Radio to TV. TV to internet news. Desktop sites to mobile apps. Mobile apps to progressive web apps.\n\nEach shift felt revolutionary. Each time, the old guard insisted their medium was irreplaceable. Newspapers said radio couldn't deliver real journalism. Radio said TV was a gimmick. Desktop advocates said mobile was too limited. They were all eventually proven wrong.\n\nAre we watching it happen again? Building browser-based AI tools while the infrastructure shifts underneath us?\n\n## What Actually Survives\n\nHere's the nuance everyone's missing: MCP doesn't kill all web apps. It kills _bad_ web apps. The thin wrappers around AI capabilities. The ones that are just a pretty chat interface with some prompts. The ones that don't provide real value beyond access. If your product is genuinely solving a complex problem, creating unique workflows, or building network effects, you're not just safe. You're better positioned. Now you can reach users through multiple channels: your web app, your API, and through MCP servers that AI agents connect to.\n\nThe question is: which category are you in?\n\n## What This Means for Builders\n\nYour core value needs to run deeper than \"we made AI easy to use.\" That's table stakes now. And it's getting easier every quarter as the models improve. Here's what I'm doing differently:\n\n**Building MCP servers alongside web apps.** Not as an afterthought. From day one. If I'm building something users should access through an AI agent, I'm making that path as smooth as the web interface.\n\n**Focusing on workflows that are hard to replicate.** Simple Q&A? That's commodity. Complex multi-step processes with state management and business logic? That's defensible.\n\n**Accepting that distribution is changing.** The metric isn't just \"monthly active users visiting our site.\" It's \"services connected through our infrastructure.\" That's a mental shift, but it's necessary.\n\n## The Reality Check\n\nModels improve every quarter. What requires a custom interface today might work through a simple conversation in six months.\n\nI've seen this firsthand. Features I built elaborate UIs for six months ago? Users now just describe what they want in plain English, and the AI figures it out. If your entire moat is UI/UX around AI capabilities, that moat is evaporating. Ask yourself honestly: \"If users could access this through their AI agent without ever opening my app, would they?\"\n\nIf the answer is yes, you might be building temporary infrastructure.\n\n## The Uncomfortable Truth\n\nI'm not saying stop building web apps. I still build them. But I'm asking harder questions about what layer I'm building on.\n\nAm I creating lasting value, or am I just filling time until the next platform shift? Am I building the Dropbox of AI tools (essential infrastructure), or am I building another note-taking app that'll be replaced by a native feature?\n\nThe winners in this shift won't be the ones with the prettiest AI chat interface. They'll be the ones that become essential infrastructure that agents need to connect to.\n\n## What's Next\n\nWe're moving past \"there's an app for that.\" Soon, apps and protocols might both become irrelevant. AI will just understand what you want and make it happen, connecting to whatever services it needs behind the scenes.\n\nYour job as a builder? Make sure your service is one it needs to connect to. Not because you have the best landing page or the smoothest onboarding flow. But because you solve a real problem that can't be replicated by a smarter model and a better prompt.\n\nThat's the bet I'm making. And honestly? I'm not sure if I'm early or late. But I'd rather ask these questions now than in 18 months when the answer is obvious.\n\nAre you ready?",
    "description": "MCP is shifting how users access AI services, and your carefully designed web app might become optional sooner than you think.",
    "publishedAt": "2025-10-15",
    "title": "Are We Building Tomorrow's Legacy Code?",
    "_meta": {
      "filePath": "building-legacy-code.mdx",
      "fileName": "building-legacy-code.mdx",
      "directory": ".",
      "extension": "mdx",
      "path": "building-legacy-code"
    },
    "body": "Here's an uncomfortable question I've been sitting with lately.\n\nWe're all racing to build AI-powered web apps. Chatbots, document processors, code assistants, productivity tools. The market is exploding, and everyone wants a piece. But what if we're building on the wrong layer?\n\n## The Shift Happening Right Now\n\nMCP (Model Context Protocol) is changing the game. Instead of users opening your web app, logging in, and using your interface, they can now connect directly to AI agents that access your functionality through MCP servers.\n\nThink about what that means:\n\n- No more context switching between apps\n- No more copying and pasting between tools\n- No more \"let me open that other tab\"\n\nThe AI agent becomes the interface. Your carefully designed UI? Your onboarding flow? Your sticky features that keep users coming back? They might all become optional.\n\nWe've already seen glimpses of this with ChatGPT Apps and custom GPTs. Users don't visit separate websites anymore. They just talk to their AI agent, and it handles the rest.\n\n> The best interface is no interface by [Golden Krishna](https://x.com/goldenkrishna)\n\n## A Pattern We've Seen Before\n\nRemember when every business needed a mobile app? Newspapers gave way to radio. Radio to TV. TV to internet news. Desktop sites to mobile apps. Mobile apps to progressive web apps.\n\nEach shift felt revolutionary. Each time, the old guard insisted their medium was irreplaceable. Newspapers said radio couldn't deliver real journalism. Radio said TV was a gimmick. Desktop advocates said mobile was too limited. They were all eventually proven wrong.\n\nAre we watching it happen again? Building browser-based AI tools while the infrastructure shifts underneath us?\n\n## What Actually Survives\n\nHere's the nuance everyone's missing: MCP doesn't kill all web apps. It kills _bad_ web apps. The thin wrappers around AI capabilities. The ones that are just a pretty chat interface with some prompts. The ones that don't provide real value beyond access. If your product is genuinely solving a complex problem, creating unique workflows, or building network effects, you're not just safe. You're better positioned. Now you can reach users through multiple channels: your web app, your API, and through MCP servers that AI agents connect to.\n\nThe question is: which category are you in?\n\n## What This Means for Builders\n\nYour core value needs to run deeper than \"we made AI easy to use.\" That's table stakes now. And it's getting easier every quarter as the models improve. Here's what I'm doing differently:\n\n**Building MCP servers alongside web apps.** Not as an afterthought. From day one. If I'm building something users should access through an AI agent, I'm making that path as smooth as the web interface.\n\n**Focusing on workflows that are hard to replicate.** Simple Q&A? That's commodity. Complex multi-step processes with state management and business logic? That's defensible.\n\n**Accepting that distribution is changing.** The metric isn't just \"monthly active users visiting our site.\" It's \"services connected through our infrastructure.\" That's a mental shift, but it's necessary.\n\n## The Reality Check\n\nModels improve every quarter. What requires a custom interface today might work through a simple conversation in six months.\n\nI've seen this firsthand. Features I built elaborate UIs for six months ago? Users now just describe what they want in plain English, and the AI figures it out. If your entire moat is UI/UX around AI capabilities, that moat is evaporating. Ask yourself honestly: \"If users could access this through their AI agent without ever opening my app, would they?\"\n\nIf the answer is yes, you might be building temporary infrastructure.\n\n## The Uncomfortable Truth\n\nI'm not saying stop building web apps. I still build them. But I'm asking harder questions about what layer I'm building on.\n\nAm I creating lasting value, or am I just filling time until the next platform shift? Am I building the Dropbox of AI tools (essential infrastructure), or am I building another note-taking app that'll be replaced by a native feature?\n\nThe winners in this shift won't be the ones with the prettiest AI chat interface. They'll be the ones that become essential infrastructure that agents need to connect to.\n\n## What's Next\n\nWe're moving past \"there's an app for that.\" Soon, apps and protocols might both become irrelevant. AI will just understand what you want and make it happen, connecting to whatever services it needs behind the scenes.\n\nYour job as a builder? Make sure your service is one it needs to connect to. Not because you have the best landing page or the smoothest onboarding flow. But because you solve a real problem that can't be replicated by a smarter model and a better prompt.\n\nThat's the bet I'm making. And honestly? I'm not sure if I'm early or late. But I'd rather ask these questions now than in 18 months when the answer is obvious.\n\nAre you ready?",
    "slug": "building-legacy-code"
  },
  {
    "content": "With Code Splitting we are able to reduce kilobytes sent over the wire and potentially improve page speed. Code Splitting is a such an easy-to-use feature that every single page app should consider to add (if haven't yet).\n\n## So why disable Code Splitting?\n\nWell, of course we shouldn't!\n\nIf you are building server-side rendered apps in React with [Code Splitting](https://webpack.js.org/guides/code-splitting/) support baked in, some might use zero-configuration tools like [Razzle](https://github.com/jaredpalmer/razzle), which runs two webpack instances, one for the client and the other for the server, and it builds everything for the client and for the server for you.\n\n## Let's build an app with Code Splitting\n\nAfter defining some major split points in your app, you want to build and see how the chunks are doing so far:\n\n<Image\n  src=\"/images/code-splitting/build.png\"\n  width={800}\n  height={619}\n  alt=\"Webpack build\"\n/>\n\nAnd in the file explorer:\n\n<Image\n  src=\"/images/code-splitting/file-explorer.png\"\n  width={690}\n  height={1408}\n  alt=\"File explorer\"\n/>\n\nBut wait?\n\n<Image\n  src=\"/images/code-splitting/server-files.png\"\n  width={694}\n  height={1014}\n  alt=\"Server files\"\n/>\n\n## What is happening here?\n\nBecause our app is splitting bundles both on the client and server, multiple bundles are created for the server too, which we don't really want/need. So we end up having multiple split up _\\[number\\]_._server.js._ What we want is:\n\nüìù One single server.js.\n\n## Let's fix it\n\nOne way would be to create another build step to delete all \\[number\\].server.js, but that would be the last (or not an) option.\n\nA better way would be to tweak the webpack config to _disable Code Splitting for the server_. Doing some research I came across [this gist](https://gist.github.com/jcenturion/892c718abce234243a156255f8f52468) by [@jcenturion86](https://x.com/jcenturion86) and learned about [LimitChunkCountPlugin](https://webpack.js.org/plugins/limit-chunk-count-plugin) that allows me to set a limit ofhow many chunks should be created*.* So the config looks like this:\n\n```js\nmodule.exports = {\n  plugins: [\n    new webpack.optimize.LimitChunkCountPlugin({\n      maxChunks: 1,\n    }),\n  ],\n};\n```\n\nWith this, one single chunk/bundle will be created and voila: Code Splitting is disabled.\n\nSo in Razzle, we want to set it only for the server by doing this in the razzle.config.js:\n\n```js\nmodule.exports = {\n  module: {\n    rules: [\n      {\n        test: /\\.(js|jsx)$/,\n        loader: require.resolve('babel-loader'),\n        include: ['src'],\n        options: {\n          presets: [\n            'dynamic-import-node',\n            'remove-webpack',\n          ],\n        }\n      ],\n    },\n  },\n};\n```\n\nSome people encountered type or runtime errors with this approach. So alternatively, we could add these two babel plugins: [babel-plugin-dynamic-import-node](https://github.com/airbnb/babel-plugin-dynamic-import-node) to transpile _import()_ to a deferred _require()_ and then remove webpack features with [babel-plugin-remove-webpack](https://github.com/knpwrs/babel-plugin-remove-webpack):\n\nBoth solutions solve the same issue:\n\n<Image\n  src=\"/images/code-splitting/server-file.png\"\n  width={546}\n  height={258}\n  alt=\"Server file\"\n/>\n\n‚úÖ One single server.js",
    "description": "Code Splitting cuts KBs sent & boosts page speed. An easy feature every single page app should consider adding if not already.",
    "publishedAt": "2017-10-21",
    "title": "How to disable Code Splitting in webpack",
    "_meta": {
      "filePath": "code-splitting.mdx",
      "fileName": "code-splitting.mdx",
      "directory": ".",
      "extension": "mdx",
      "path": "code-splitting"
    },
    "body": "With Code Splitting we are able to reduce kilobytes sent over the wire and potentially improve page speed. Code Splitting is a such an easy-to-use feature that every single page app should consider to add (if haven't yet).\n\n## So why disable Code Splitting?\n\nWell, of course we shouldn't!\n\nIf you are building server-side rendered apps in React with [Code Splitting](https://webpack.js.org/guides/code-splitting/) support baked in, some might use zero-configuration tools like [Razzle](https://github.com/jaredpalmer/razzle), which runs two webpack instances, one for the client and the other for the server, and it builds everything for the client and for the server for you.\n\n## Let's build an app with Code Splitting\n\nAfter defining some major split points in your app, you want to build and see how the chunks are doing so far:\n\n<Image\n  src=\"/images/code-splitting/build.png\"\n  width={800}\n  height={619}\n  alt=\"Webpack build\"\n/>\n\nAnd in the file explorer:\n\n<Image\n  src=\"/images/code-splitting/file-explorer.png\"\n  width={690}\n  height={1408}\n  alt=\"File explorer\"\n/>\n\nBut wait?\n\n<Image\n  src=\"/images/code-splitting/server-files.png\"\n  width={694}\n  height={1014}\n  alt=\"Server files\"\n/>\n\n## What is happening here?\n\nBecause our app is splitting bundles both on the client and server, multiple bundles are created for the server too, which we don't really want/need. So we end up having multiple split up _\\[number\\]_._server.js._ What we want is:\n\nüìù One single server.js.\n\n## Let's fix it\n\nOne way would be to create another build step to delete all \\[number\\].server.js, but that would be the last (or not an) option.\n\nA better way would be to tweak the webpack config to _disable Code Splitting for the server_. Doing some research I came across [this gist](https://gist.github.com/jcenturion/892c718abce234243a156255f8f52468) by [@jcenturion86](https://x.com/jcenturion86) and learned about [LimitChunkCountPlugin](https://webpack.js.org/plugins/limit-chunk-count-plugin) that allows me to set a limit ofhow many chunks should be created*.* So the config looks like this:\n\n```js\nmodule.exports = {\n  plugins: [\n    new webpack.optimize.LimitChunkCountPlugin({\n      maxChunks: 1,\n    }),\n  ],\n};\n```\n\nWith this, one single chunk/bundle will be created and voila: Code Splitting is disabled.\n\nSo in Razzle, we want to set it only for the server by doing this in the razzle.config.js:\n\n```js\nmodule.exports = {\n  module: {\n    rules: [\n      {\n        test: /\\.(js|jsx)$/,\n        loader: require.resolve('babel-loader'),\n        include: ['src'],\n        options: {\n          presets: [\n            'dynamic-import-node',\n            'remove-webpack',\n          ],\n        }\n      ],\n    },\n  },\n};\n```\n\nSome people encountered type or runtime errors with this approach. So alternatively, we could add these two babel plugins: [babel-plugin-dynamic-import-node](https://github.com/airbnb/babel-plugin-dynamic-import-node) to transpile _import()_ to a deferred _require()_ and then remove webpack features with [babel-plugin-remove-webpack](https://github.com/knpwrs/babel-plugin-remove-webpack):\n\nBoth solutions solve the same issue:\n\n<Image\n  src=\"/images/code-splitting/server-file.png\"\n  width={546}\n  height={258}\n  alt=\"Server file\"\n/>\n\n‚úÖ One single server.js",
    "slug": "code-splitting"
  },
  {
    "content": "Great design speaks before the logo ever does. Certain products feel familiar long before you read the name.\n\nThink of Apple with its curved, flawless material choices, restrained packaging, and the way everything feels intentional from lid to charging cable. Or Xiaomi with its clean lines, balanced proportions, and simple color stories that still feel premium. You notice those fingerprints before the branding. That is a design language doing its job. It is the visual tone and character that connects every piece of the experience.\n\n> Details are not the details. They make the design.\n\nThe same rhythm applies on the web. Every site or app we ship carries a language whether we define it or not. When we get it right, people recognize the product before they see a logo or read a headline.\n\nDesign language is the promise that everything in the experience belongs together. It can be calm and warm, precise and technical, or playful and bright. Whatever the character, we feel it before we read copy or spot branding.\n\n## Language Before Components\n\nDesign language works like a shared vocabulary. It collects principles, tone, and the emotional notes we want every screen, sound, or animation to carry. When a team documents that vocabulary, components stop feeling like disconnected parts and start behaving like sentences from the same author.\n\n> Good design is honest.\n\n## From Design Language to Design System\n\nA design system exists to translate the language into practical assets. Tokens, component libraries, content guidelines, even accessibility checklists. They are all carriers for the language. Keep the translation minimal and make sure the product stays recognizable.\n\nA few checkpoints:\n\n- Write down the personality traits and principles the product should express\n- Connect each principle to tangible decisions like color tokens, spacing scales, typography, or motion\n- Document patterns showing how those decisions appear in real flows\n- Review new work against the language before introducing brand-new variants\n\n## Bridging Design Language in Code\n\nDesign engineers keep the language alive once it leaves Figma. Tokens are mirrored into typed modules, baked into a global config, and then component props enforce the tone. Documentation helps, but baked-in constraints inside the repo do most of the work.\n\n> Design is intelligence made visible.\n\nWhat are your favourite products with a design language you felt before you saw the logo?",
    "description": "Great design speaks before the logo ever does. Certain products feel familiar long before you read the name.",
    "publishedAt": "2025-10-21",
    "title": "When Your Design System Finds Its Language",
    "_meta": {
      "filePath": "design-language.mdx",
      "fileName": "design-language.mdx",
      "directory": ".",
      "extension": "mdx",
      "path": "design-language"
    },
    "body": "Great design speaks before the logo ever does. Certain products feel familiar long before you read the name.\n\nThink of Apple with its curved, flawless material choices, restrained packaging, and the way everything feels intentional from lid to charging cable. Or Xiaomi with its clean lines, balanced proportions, and simple color stories that still feel premium. You notice those fingerprints before the branding. That is a design language doing its job. It is the visual tone and character that connects every piece of the experience.\n\n> Details are not the details. They make the design.\n\nThe same rhythm applies on the web. Every site or app we ship carries a language whether we define it or not. When we get it right, people recognize the product before they see a logo or read a headline.\n\nDesign language is the promise that everything in the experience belongs together. It can be calm and warm, precise and technical, or playful and bright. Whatever the character, we feel it before we read copy or spot branding.\n\n## Language Before Components\n\nDesign language works like a shared vocabulary. It collects principles, tone, and the emotional notes we want every screen, sound, or animation to carry. When a team documents that vocabulary, components stop feeling like disconnected parts and start behaving like sentences from the same author.\n\n> Good design is honest.\n\n## From Design Language to Design System\n\nA design system exists to translate the language into practical assets. Tokens, component libraries, content guidelines, even accessibility checklists. They are all carriers for the language. Keep the translation minimal and make sure the product stays recognizable.\n\nA few checkpoints:\n\n- Write down the personality traits and principles the product should express\n- Connect each principle to tangible decisions like color tokens, spacing scales, typography, or motion\n- Document patterns showing how those decisions appear in real flows\n- Review new work against the language before introducing brand-new variants\n\n## Bridging Design Language in Code\n\nDesign engineers keep the language alive once it leaves Figma. Tokens are mirrored into typed modules, baked into a global config, and then component props enforce the tone. Documentation helps, but baked-in constraints inside the repo do most of the work.\n\n> Design is intelligence made visible.\n\nWhat are your favourite products with a design language you felt before you saw the logo?",
    "slug": "design-language"
  },
  {
    "content": "I recently did something unusual on my personal site: I made almost every piece of text the same font size. No oversized headings, no tiny captions. Just one size everywhere.\n\nIt sounds like a small tweak, but it completely changed the feel of the site and how I think about typography in web design.\n\n<Image\n  src=\"/images/font-size/home.png\"\n  width={1920}\n  height={1080}\n  alt=\"Home page\"\n/>\n\n## Why I Chose One Font Size for My Website\n\nDesign usually relies on different font sizes to create hierarchy: big titles, medium subtitles, and smaller body text. But on my own site, I wanted something calmer. Instead of a corporate or marketing look, I wanted it to feel more like a personal journal.\n\nUsing a single font size gave the site a rhythm that feels simpler and more intentional. It doesn‚Äôt shout with big headings. It just flows while being pleasant to scroll through.\n\n## Benefits of Minimalist Typography\n\nSwitching to one font size surprised me in a good way. Here‚Äôs what I like about it:\n\n- **It‚Äôs minimal.** Everything feels balanced, without text competing for attention.\n- **It feels confident.** The words carry the weight, not the formatting.\n- **It changes how I write.** Without relying on big titles, I focus more on structure and clarity.\n\nIn short, the design feels cleaner, more personal, and more human.\n\n<Image\n  src=\"/images/font-size/about.png\"\n  width={1920}\n  height={1080}\n  alt=\"Font size\"\n/>\n\n## Challenges of Using the Same Font Size Everywhere\n\nOf course, this approach has its drawbacks:\n\n- **Scanning is harder.** Without larger headings, readers can‚Äôt skim as quickly.\n- **Navigation can blur.** Menus and content risk looking too similar.\n- **Accessibility matters.** One size has to be chosen carefully so text is easy to read.\n\nSo while it works for a personal blog, I wouldn‚Äôt recommend it for every site. Especially ones that rely on quick navigation or complex layouts.\n\n## What I Learned About Web Design\n\nThis experiment taught me that design isn‚Äôt always about adding more. Sometimes it‚Äôs about simplicity. Removing font-size hierarchy forced me to think about whitespace, rhythm, and tone.\n\nIt reminded me that good web design is about intention. For my personal site, one consistent font size feels calm, clear, and aligned with who I am.\n\n### Closing Thought\n\nDesign choices don‚Äôt just change how a website looks. They change how it _feels_. By using a single font size, I made my site less like a presentation and more like a conversation. And that feels right for me.",
    "description": "I redesigned my site with one font size. Here‚Äôs why minimalist typography works, what I love about it, and when it falls short.",
    "publishedAt": "2025-09-10",
    "title": "One Font Size",
    "_meta": {
      "filePath": "font-size.mdx",
      "fileName": "font-size.mdx",
      "directory": ".",
      "extension": "mdx",
      "path": "font-size"
    },
    "body": "I recently did something unusual on my personal site: I made almost every piece of text the same font size. No oversized headings, no tiny captions. Just one size everywhere.\n\nIt sounds like a small tweak, but it completely changed the feel of the site and how I think about typography in web design.\n\n<Image\n  src=\"/images/font-size/home.png\"\n  width={1920}\n  height={1080}\n  alt=\"Home page\"\n/>\n\n## Why I Chose One Font Size for My Website\n\nDesign usually relies on different font sizes to create hierarchy: big titles, medium subtitles, and smaller body text. But on my own site, I wanted something calmer. Instead of a corporate or marketing look, I wanted it to feel more like a personal journal.\n\nUsing a single font size gave the site a rhythm that feels simpler and more intentional. It doesn‚Äôt shout with big headings. It just flows while being pleasant to scroll through.\n\n## Benefits of Minimalist Typography\n\nSwitching to one font size surprised me in a good way. Here‚Äôs what I like about it:\n\n- **It‚Äôs minimal.** Everything feels balanced, without text competing for attention.\n- **It feels confident.** The words carry the weight, not the formatting.\n- **It changes how I write.** Without relying on big titles, I focus more on structure and clarity.\n\nIn short, the design feels cleaner, more personal, and more human.\n\n<Image\n  src=\"/images/font-size/about.png\"\n  width={1920}\n  height={1080}\n  alt=\"Font size\"\n/>\n\n## Challenges of Using the Same Font Size Everywhere\n\nOf course, this approach has its drawbacks:\n\n- **Scanning is harder.** Without larger headings, readers can‚Äôt skim as quickly.\n- **Navigation can blur.** Menus and content risk looking too similar.\n- **Accessibility matters.** One size has to be chosen carefully so text is easy to read.\n\nSo while it works for a personal blog, I wouldn‚Äôt recommend it for every site. Especially ones that rely on quick navigation or complex layouts.\n\n## What I Learned About Web Design\n\nThis experiment taught me that design isn‚Äôt always about adding more. Sometimes it‚Äôs about simplicity. Removing font-size hierarchy forced me to think about whitespace, rhythm, and tone.\n\nIt reminded me that good web design is about intention. For my personal site, one consistent font size feels calm, clear, and aligned with who I am.\n\n### Closing Thought\n\nDesign choices don‚Äôt just change how a website looks. They change how it _feels_. By using a single font size, I made my site less like a presentation and more like a conversation. And that feels right for me.",
    "slug": "font-size"
  },
  {
    "content": "If you've been following AI development lately, you might have heard about something called MCP, or the Model Context Protocol. Don't worry if it sounds technical - I'll break it down in simple terms and show you what it means for the future of AI tools.\n\n## What is MCP?\n\nThink of MCP as a \"USB port for AI.\" Just like USB allowed us to connect any device to any computer with a standard connection, MCP lets AI assistants connect to any system, tool, or data source using a standard protocol.\n\nBefore MCP, if you wanted your AI assistant to access your Google Drive, Slack messages, or GitHub repositories, each integration had to be built from scratch. With MCP, there's now a universal way for AI tools to \"plug into\" these systems.\n\n## Why Does This Matter?\n\nImagine you're working with Claude and you want to:\n\n- Search through your company's documentation\n- Check your calendar for meetings\n- Pull data from your database\n- Update a Slack channel\n\nWithout MCP, these would require separate, custom integrations. With MCP, your AI assistant can connect to all these systems through a single, standardized protocol.\n\n## My MCP Server\n\nI recently built my own MCP server for this portfolio website. You can check it out at [/mcp/about](/mcp/about). Here's what it does:\n\n**Content Management**: It gives AI assistants access to all my blog posts, talks, and workshops. An AI can now search through my content, find specific articles, or get summaries of my work.\n\n**Portfolio Data**: It provides information about my speaking appearances, upcoming events, and professional activities - all in a format that AI tools can easily understand and work with.\n\n**Newsletter Management**: It includes tools for managing newsletter campaigns, checking subscriber statistics, and creating new content for my audience.\n\nThe cool thing is that any MCP-compatible AI tool can now access this information about my work without me having to build separate integrations for each tool.\n\n## Popular MCP Examples\n\nThe MCP ecosystem has exploded since Anthropic introduced it in late 2024. Here are some popular examples you might find useful:\n\n**Development Tools**:\n\n- **Git Server** - Let AI assistants read, search, and work with your Git repositories\n- **Filesystem Server** - Give AI secure access to your local files with configurable permissions\n- **GitHub Server** - Connect AI to your GitHub issues, pull requests, and repositories\n\n**Business & Productivity**:\n\n- **Slack Server** - AI can read messages, send updates, and manage Slack workflows\n- **Google Drive Server** - Access and manage your Google Drive documents\n- **Atlassian Server** - Work with Jira tickets and Confluence pages\n\n**Cloud & Data**:\n\n- **PostgreSQL Server** - Query and analyze your database directly through AI\n- **AWS Server** - Manage AWS resources and services\n- **Aiven Server** - Connect to various database services like PostgreSQL, Kafka, and ClickHouse\n\n**Finance & Trading**:\n\n- **Alpaca Server** - Trade stocks, analyze market data, and build trading strategies\n\n## The Technical Side (Simplified)\n\nMCP works with three main components:\n\n1. **Tools** - Actions the AI can perform (like sending an email or querying a database)\n2. **Resources** - Data the AI can read (like files or database records)\n3. **Prompts** - Pre-built templates that help the AI use tools and resources effectively\n\nWhen you connect an MCP server to an AI assistant, you're essentially giving it a new set of superpowers specific to that system or service.\n\n## What This Means for the Future\n\nMCP is still very new, but it's growing incredibly fast. In just a few months since its release, developers have created over 1,000 MCP servers for different services and tools.\n\nThis standardization means:\n\n- **Easier integrations** - Connect your AI to new tools without custom development\n- **Better workflows** - AI assistants can work across multiple systems seamlessly\n- **More powerful automation** - Combine different tools and data sources in sophisticated ways\n\nPopular development tools like Cursor, Windsurf, and Codeium have already integrated MCP support, making it a one-click setup to connect AI to your development environment.\n\n## Getting Started\n\nIf you want to explore MCP, you can:\n\n1. Check out the [official MCP servers repository](https://github.com/modelcontextprotocol/servers) for examples\n2. Try connecting an existing MCP server to Claude Desktop or another compatible AI tool\n3. Build your own MCP server for a service you use regularly\n\nThe protocol is designed to be developer-friendly, and there are plenty of examples to learn from.\n\n## Wrapping Up\n\nMCP might sound complex, but it's really just about making AI assistants more useful by giving them standardized ways to connect to the tools and data you already use. As more services adopt MCP, we'll see AI assistants become much more capable and integrated into our daily workflows.\n\nThe fact that we now have a universal protocol for AI integrations is exciting - it means we're moving from isolated AI tools to AI that can work seamlessly across all our systems and data sources.",
    "description": "A beginner-friendly introduction to the Model Context Protocol (MCP) - what it is, why it matters, and how I built my own MCP server for portfolio management.",
    "publishedAt": "2025-09-19",
    "title": "What is MCP?",
    "_meta": {
      "filePath": "mcp-explained.mdx",
      "fileName": "mcp-explained.mdx",
      "directory": ".",
      "extension": "mdx",
      "path": "mcp-explained"
    },
    "body": "If you've been following AI development lately, you might have heard about something called MCP, or the Model Context Protocol. Don't worry if it sounds technical - I'll break it down in simple terms and show you what it means for the future of AI tools.\n\n## What is MCP?\n\nThink of MCP as a \"USB port for AI.\" Just like USB allowed us to connect any device to any computer with a standard connection, MCP lets AI assistants connect to any system, tool, or data source using a standard protocol.\n\nBefore MCP, if you wanted your AI assistant to access your Google Drive, Slack messages, or GitHub repositories, each integration had to be built from scratch. With MCP, there's now a universal way for AI tools to \"plug into\" these systems.\n\n## Why Does This Matter?\n\nImagine you're working with Claude and you want to:\n\n- Search through your company's documentation\n- Check your calendar for meetings\n- Pull data from your database\n- Update a Slack channel\n\nWithout MCP, these would require separate, custom integrations. With MCP, your AI assistant can connect to all these systems through a single, standardized protocol.\n\n## My MCP Server\n\nI recently built my own MCP server for this portfolio website. You can check it out at [/mcp/about](/mcp/about). Here's what it does:\n\n**Content Management**: It gives AI assistants access to all my blog posts, talks, and workshops. An AI can now search through my content, find specific articles, or get summaries of my work.\n\n**Portfolio Data**: It provides information about my speaking appearances, upcoming events, and professional activities - all in a format that AI tools can easily understand and work with.\n\n**Newsletter Management**: It includes tools for managing newsletter campaigns, checking subscriber statistics, and creating new content for my audience.\n\nThe cool thing is that any MCP-compatible AI tool can now access this information about my work without me having to build separate integrations for each tool.\n\n## Popular MCP Examples\n\nThe MCP ecosystem has exploded since Anthropic introduced it in late 2024. Here are some popular examples you might find useful:\n\n**Development Tools**:\n\n- **Git Server** - Let AI assistants read, search, and work with your Git repositories\n- **Filesystem Server** - Give AI secure access to your local files with configurable permissions\n- **GitHub Server** - Connect AI to your GitHub issues, pull requests, and repositories\n\n**Business & Productivity**:\n\n- **Slack Server** - AI can read messages, send updates, and manage Slack workflows\n- **Google Drive Server** - Access and manage your Google Drive documents\n- **Atlassian Server** - Work with Jira tickets and Confluence pages\n\n**Cloud & Data**:\n\n- **PostgreSQL Server** - Query and analyze your database directly through AI\n- **AWS Server** - Manage AWS resources and services\n- **Aiven Server** - Connect to various database services like PostgreSQL, Kafka, and ClickHouse\n\n**Finance & Trading**:\n\n- **Alpaca Server** - Trade stocks, analyze market data, and build trading strategies\n\n## The Technical Side (Simplified)\n\nMCP works with three main components:\n\n1. **Tools** - Actions the AI can perform (like sending an email or querying a database)\n2. **Resources** - Data the AI can read (like files or database records)\n3. **Prompts** - Pre-built templates that help the AI use tools and resources effectively\n\nWhen you connect an MCP server to an AI assistant, you're essentially giving it a new set of superpowers specific to that system or service.\n\n## What This Means for the Future\n\nMCP is still very new, but it's growing incredibly fast. In just a few months since its release, developers have created over 1,000 MCP servers for different services and tools.\n\nThis standardization means:\n\n- **Easier integrations** - Connect your AI to new tools without custom development\n- **Better workflows** - AI assistants can work across multiple systems seamlessly\n- **More powerful automation** - Combine different tools and data sources in sophisticated ways\n\nPopular development tools like Cursor, Windsurf, and Codeium have already integrated MCP support, making it a one-click setup to connect AI to your development environment.\n\n## Getting Started\n\nIf you want to explore MCP, you can:\n\n1. Check out the [official MCP servers repository](https://github.com/modelcontextprotocol/servers) for examples\n2. Try connecting an existing MCP server to Claude Desktop or another compatible AI tool\n3. Build your own MCP server for a service you use regularly\n\nThe protocol is designed to be developer-friendly, and there are plenty of examples to learn from.\n\n## Wrapping Up\n\nMCP might sound complex, but it's really just about making AI assistants more useful by giving them standardized ways to connect to the tools and data you already use. As more services adopt MCP, we'll see AI assistants become much more capable and integrated into our daily workflows.\n\nThe fact that we now have a universal protocol for AI integrations is exciting - it means we're moving from isolated AI tools to AI that can work seamlessly across all our systems and data sources.",
    "slug": "mcp-explained"
  },
  {
    "content": "The web is getting more cinematic. With React's ViewTransition component, you can create smooth, native page transitions that feel like mobile apps without complex animation libraries.\n\n## The Vanilla JS Way\n\nThe browser's ViewTransition API requires manual DOM manipulation:\n\n```js\n// Vanilla JavaScript approach\nfunction navigateToPost(postId) {\n  // Start the transition\n  document.startViewTransition(() => {\n    // Update the DOM\n    document.getElementById('content').innerHTML = getPostContent(postId);\n  });\n}\n```\n\nThis works, but it's verbose and requires managing the DOM directly.\n\n## The React Way\n\nReact's `unstable_ViewTransition` component makes it dead simple, but you need to trigger it with `startTransition`:\n\n```tsx\nimport { unstable_ViewTransition as ViewTransition } from 'react';\nimport { startTransition } from 'react';\n\nfunction PostTitle({ title, slug }) {\n  return (\n    <ViewTransition name={`post-${slug}`}>\n      <h1>{title}</h1>\n    </ViewTransition>\n  );\n}\n\n// Trigger the transition\nfunction navigateToPost(postSlug) {\n  startTransition(() => {\n    router.push(`/posts/${postSlug}`);\n  });\n}\n```\n\nThe `ViewTransition` component marks which elements should animate. The `startTransition` wrapper tells React when to activate those animations.\n\n## What Are View Transitions?\n\nView transitions create smooth animations between different states of your app. Think of how iOS apps animate between screens: that's what you get on the web now.\n\nThe browser automatically creates a transition between the \"old\" and \"new\" states, morphing elements it can match between the two renders.\n\n### How Transitions Are Triggered\n\nReact's view transitions only activate when you wrap state updates in a `startTransition`:\n\n```tsx\nimport { startTransition } from 'react';\n\n// ‚úÖ This triggers view transitions\nstartTransition(() => {\n  setActivePost(newPost);\n});\n\n// ‚ùå This does NOT trigger view transitions\nsetActivePost(newPost);\n```\n\nFor navigation, Next.js Link components automatically use `startTransition` internally, so clicking links will trigger view transitions without extra code.\n\n## Putting It Together\n\nAfter trying a bunch of helpers I ended up keeping the setup simple. Wrap the element you want to animate with `ViewTransition`, and trigger navigation inside `startTransition`:\n\n```tsx\nimport type { ReactNode } from 'react';\nimport {\n  unstable_ViewTransition as ViewTransition,\n  startTransition,\n} from 'react';\nimport { useRouter } from 'next/navigation';\n\nfunction PostTitle({ children }: { children: ReactNode }) {\n  return (\n    <ViewTransition name=\"post-title\">\n      <h1>{children}</h1>\n    </ViewTransition>\n  );\n}\n\nfunction usePostNavigation() {\n  const router = useRouter();\n\n  return (slug: string) => {\n    startTransition(() => {\n      router.push(`/posts/${slug}`);\n    });\n  };\n}\n```\n\nThat‚Äôs the whole recipe‚ÄîReact handles wiring the browser API for you, so there‚Äôs no need for extra CSS selectors or custom link components. If you want multiple elements to animate, wrap each one in `ViewTransition` and give it a unique `name`.\n\n## Next.js Configuration\n\nEnable the experimental feature:\n\n```tsx\n// next.config.ts\nconst nextConfig = {\n  experimental: {\n    viewTransition: true,\n  },\n};\n\nexport default nextConfig;\n```\n\n## Why This Matters\n\nView transitions solve a real problem. Traditional page navigation feels jarring: content just pops in and out. With view transitions, users get visual continuity that makes your app feel polished.\n\nThe best part? It's progressive enhancement. Browsers that don't support it just show normal navigation.\n\n## Implementation Tips\n\n1. **Use consistent naming**: `post-${slug}`, `talk-${slug}`, etc.\n2. **Start simple**: Focus on titles and key elements first\n3. **Test across browsers**: Chrome has the best support currently\n4. **Keep it subtle**: Over-animation can feel gimmicky\n\n## The Future\n\nThis is just the beginning. React's ViewTransition integration opens up possibilities for:\n\n- Smooth route changes in SPAs\n- Gallery transitions\n- Modal animations\n- List reordering effects\n\n## Getting Started\n\nWant to try it? Use React's ViewTransition component and start with your most important page transitions. The API is straightforward, and the results are immediately satisfying.\n\nThe web is becoming more app-like, and React's ViewTransition integration is a big step in that direction. Your users will notice the difference.\n\n---\n\nüëâ **Bottom line**: React's ViewTransition integration makes smooth page animations trivial. It's time to make your app feel more native.\n\n## Source Code\n\nYou can explore the complete implementation in the [glennreyes.com repository](https://github.com/glennreyes/glennreyes.com). The ViewTransition integration was added in [this pull request](https://github.com/glennreyes/glennreyes.com/pull/613) with all the changes shown above.",
    "description": "Smooth page animations with React's ViewTransition component. Examples and implementation guide.",
    "publishedAt": "2025-10-09",
    "title": "View Transition in React",
    "_meta": {
      "filePath": "react-view-transitions.mdx",
      "fileName": "react-view-transitions.mdx",
      "directory": ".",
      "extension": "mdx",
      "path": "react-view-transitions"
    },
    "body": "The web is getting more cinematic. With React's ViewTransition component, you can create smooth, native page transitions that feel like mobile apps without complex animation libraries.\n\n## The Vanilla JS Way\n\nThe browser's ViewTransition API requires manual DOM manipulation:\n\n```js\n// Vanilla JavaScript approach\nfunction navigateToPost(postId) {\n  // Start the transition\n  document.startViewTransition(() => {\n    // Update the DOM\n    document.getElementById('content').innerHTML = getPostContent(postId);\n  });\n}\n```\n\nThis works, but it's verbose and requires managing the DOM directly.\n\n## The React Way\n\nReact's `unstable_ViewTransition` component makes it dead simple, but you need to trigger it with `startTransition`:\n\n```tsx\nimport { unstable_ViewTransition as ViewTransition } from 'react';\nimport { startTransition } from 'react';\n\nfunction PostTitle({ title, slug }) {\n  return (\n    <ViewTransition name={`post-${slug}`}>\n      <h1>{title}</h1>\n    </ViewTransition>\n  );\n}\n\n// Trigger the transition\nfunction navigateToPost(postSlug) {\n  startTransition(() => {\n    router.push(`/posts/${postSlug}`);\n  });\n}\n```\n\nThe `ViewTransition` component marks which elements should animate. The `startTransition` wrapper tells React when to activate those animations.\n\n## What Are View Transitions?\n\nView transitions create smooth animations between different states of your app. Think of how iOS apps animate between screens: that's what you get on the web now.\n\nThe browser automatically creates a transition between the \"old\" and \"new\" states, morphing elements it can match between the two renders.\n\n### How Transitions Are Triggered\n\nReact's view transitions only activate when you wrap state updates in a `startTransition`:\n\n```tsx\nimport { startTransition } from 'react';\n\n// ‚úÖ This triggers view transitions\nstartTransition(() => {\n  setActivePost(newPost);\n});\n\n// ‚ùå This does NOT trigger view transitions\nsetActivePost(newPost);\n```\n\nFor navigation, Next.js Link components automatically use `startTransition` internally, so clicking links will trigger view transitions without extra code.\n\n## Putting It Together\n\nAfter trying a bunch of helpers I ended up keeping the setup simple. Wrap the element you want to animate with `ViewTransition`, and trigger navigation inside `startTransition`:\n\n```tsx\nimport type { ReactNode } from 'react';\nimport {\n  unstable_ViewTransition as ViewTransition,\n  startTransition,\n} from 'react';\nimport { useRouter } from 'next/navigation';\n\nfunction PostTitle({ children }: { children: ReactNode }) {\n  return (\n    <ViewTransition name=\"post-title\">\n      <h1>{children}</h1>\n    </ViewTransition>\n  );\n}\n\nfunction usePostNavigation() {\n  const router = useRouter();\n\n  return (slug: string) => {\n    startTransition(() => {\n      router.push(`/posts/${slug}`);\n    });\n  };\n}\n```\n\nThat‚Äôs the whole recipe‚ÄîReact handles wiring the browser API for you, so there‚Äôs no need for extra CSS selectors or custom link components. If you want multiple elements to animate, wrap each one in `ViewTransition` and give it a unique `name`.\n\n## Next.js Configuration\n\nEnable the experimental feature:\n\n```tsx\n// next.config.ts\nconst nextConfig = {\n  experimental: {\n    viewTransition: true,\n  },\n};\n\nexport default nextConfig;\n```\n\n## Why This Matters\n\nView transitions solve a real problem. Traditional page navigation feels jarring: content just pops in and out. With view transitions, users get visual continuity that makes your app feel polished.\n\nThe best part? It's progressive enhancement. Browsers that don't support it just show normal navigation.\n\n## Implementation Tips\n\n1. **Use consistent naming**: `post-${slug}`, `talk-${slug}`, etc.\n2. **Start simple**: Focus on titles and key elements first\n3. **Test across browsers**: Chrome has the best support currently\n4. **Keep it subtle**: Over-animation can feel gimmicky\n\n## The Future\n\nThis is just the beginning. React's ViewTransition integration opens up possibilities for:\n\n- Smooth route changes in SPAs\n- Gallery transitions\n- Modal animations\n- List reordering effects\n\n## Getting Started\n\nWant to try it? Use React's ViewTransition component and start with your most important page transitions. The API is straightforward, and the results are immediately satisfying.\n\nThe web is becoming more app-like, and React's ViewTransition integration is a big step in that direction. Your users will notice the difference.\n\n---\n\nüëâ **Bottom line**: React's ViewTransition integration makes smooth page animations trivial. It's time to make your app feel more native.\n\n## Source Code\n\nYou can explore the complete implementation in the [glennreyes.com repository](https://github.com/glennreyes/glennreyes.com). The ViewTransition integration was added in [this pull request](https://github.com/glennreyes/glennreyes.com/pull/613) with all the changes shown above.",
    "slug": "react-view-transitions"
  },
  {
    "content": "Having attended many tech events over the past years, I have always admired people who showed off their musical skills on stage. Few examples:\n\n- [Beats in the Browser](https://youtu.be/z7EQtSK3QHM?feature=shared) by [Ken Wheeler](https://twitter.com/ken_wheeler)\n- [Alive and Kicking. A Vue into Rock& Roll!](https://youtu.be/-4m4TIJ0z20?feature=shared) by [Tim Benniks](https://twitter.com/timbenniks)\n- [Beats in the Browser - Coding Music with JavaScript](https://youtu.be/KtVILY90t4g?feature=shared) by [Rowdy Rabouw](https://twitter.com/RowdyRabouw)\n\nAs someone enthusiastic about playing guitar and making music, I have dreamed of building a fully-fledged web-based DAW (Digital Audio Workstation), but I had never looked into the Web Audio API to see what it's capable of. So, I took some time and thought, why not start with something small and build a tuner for musical instruments?\n\n## Web Audio API\n\nThe Web Audio API is a powerful JavaScript API that allows developers to do a lot of things, but most notably: creating and manipulating audio in the browser. It is a very powerful API for creating music, sound effects, and other audio applications.\n\nBefore I had any code written, I knew I would need to get a microphone input and then work from there:\n\n```tsx\nconst context = new AudioContext();\nconst stream = await navigator.mediaDevices.getUserMedia({ audio: true });\nconst microphone = context.createMediaStreamSource(stream);\n```\n\n1. `const context = new AudioContext();` - Represents an audio-processing graph built from audio modules linked together. The `AudioContext` is the main entry point to the Web Audio API and is used to create and manage audio nodes, handle audio processing, and control audio playback.\n2. `const stream = await navigator.mediaDevices.getUserMedia({ audio: true });` - Prompts the user for permission to use a media input that produces audio (such as a microphone). It returns a `Promise` of a `MediaStream` representing the user's audio input.\n3. `const microphone = context.createMediaStreamSource(stream);` - Creates an `AudioNode` from the user's audio stream obtained in the previous step. This `AudioNode` can then be connected to other nodes in the `AudioContext` for processing, analysis, or output.\n\nNow, with `microphone`, we can't really do anything helpful yet, so we need to add an `AnalyserNode` to the `AudioContext` to get the frequency data from the microphone input.\n\n<Image\n  className=\"bg-white\"\n  src=\"/images/tuner/analyser-node.svg\"\n  width={752}\n  height={224}\n  alt=\"AnalyserNode\"\n/>\n\nThat's how it looks like in code:\n\n```tsx {5-9}\nconst context = new AudioContext();\nconst stream = await navigator.mediaDevices.getUserMedia({ audio: true });\nconst microphone = context.createMediaStreamSource(stream);\n\nconst analyser = context.createAnalyser();\nmicrophone.connect(analyser);\n\nconst dataArray = new Float32Array(analyser.frequencyBinCount);\nanalyser.getFloatFrequencyData(dataArray);\n```\n\n1. We create an `AnalyserNode` using `context.createAnalyser()`.\n2. We connect the `microphone` to the `analyser` using `microphone.connect(analyser)`.\n3. We create a `Float32Array` to hold the frequency data represented as FFT (Fast Fourier Transform) data.\n4. We call `analyser.getFloatFrequencyData(dataArray)` to copy the current frequency data into `dataArray`, where each value is a sample represented as a decibel value for a particular frequency.\n\n<Image\n  className=\"mx-auto\"\n  src=\"/images/tuner/data-array.png\"\n  width={642}\n  height={384}\n  alt=\"Example of dataArray values\"\n/>\n\nSometimes it can contain `-Infinity`, which means that the sample is silent.\n\nNow that we have a \"snapshot\" of frequency data, we want to capture frequency data over time by using `requestAnimationFrame`:\n\n```tsx {10-16}\nconst context = new AudioContext();\nconst stream = await navigator.mediaDevices.getUserMedia({ audio: true });\nconst microphone = context.createMediaStreamSource(stream);\n\nconst analyser = context.createAnalyser();\nanalyser.fftSize = 2048;\nmicrophone.connect(analyser);\n\nconst dataArray = new Float32Array(analyser.frequencyBinCount);\nconst getData = () => {\n  analyser.getFloatFrequencyData(dataArray);\n\n  requestAnimationFrame(getData);\n};\n\ngetData();\n```\n\n## Capturing the Pitch\n\nOnce we have that, we want to capture the most dominant sample from the frequency data. A perfect job for looping through `dataArray` with `.reduce` and capturing the one with the highest value.\n\nSince these are represented in decibels, we specify a range of what we want to capture from, the minimum and maximum volume.\n\nFinally, we capture the most dominant sample and convert it to the actually frequency (or pitch), as each index of `dataArray` represents a frequency of half the sample rate. So, we can calculate the frequency by `(index * context.sampleRate) / analyser.fftSize`.\n\nHere's how it looks:\n\n```tsx {11-12, 17-30}\nconst context = new AudioContext();\nconst stream = await navigator.mediaDevices.getUserMedia({ audio: true });\nconst microphone = context.createMediaStreamSource(stream);\n\nconst analyser = context.createAnalyser();\nanalyser.fftSize = 2048;\nmicrophone.connect(analyser);\n\nconst dataArray = new Float32Array(analyser.frequencyBinCount);\n\nconst minDecibels = -60; // Minimum decibels representing 0% volume\nconst maxDecibels = 0; // Maximum decibels representing 100% volume\n\nconst getData = () => {\n  analyser.getFloatFrequencyData(dataArray);\n\n  const data = dataArray.reduce(\n    (acc, decibels, index) => {\n      const volume =\n        ((decibels - minDecibels) / (maxDecibels - minDecibels)) * 100;\n\n      if (volume > acc.maxVolume) {\n        return { maxIndex: index, maxVolume: volume };\n      }\n\n      return acc;\n    },\n    { maxIndex: -1, maxVolume: 0 },\n  );\n  const frequency = (data.maxIndex * context.sampleRate) / analyser.fftSize;\n\n  // Do something with `frequency` ...\n\n  requestAnimationFrame(getData);\n};\n\ngetData();\n```\n\n## From Pitch to Note\n\nMapping a pitch to note is fairly simple. Before I go more into that, I think it's important to know about the reference pitch of 440 Hertz, also known as standard pitch, concert pitch or A440.\n\n### The Pitch Standard\n\nThe pitch standard, also known as A440, is a very common musical pitch standard that specifies the frequency of the A above middle C on the piano as 440 Hz:\n\n<Image\n  className=\"bg-white p-4\"\n  src=\"/images/tuner/a440.svg\"\n  width={752}\n  height={171}\n  alt=\"AnalyserNode\"\n/>\n\nHaving this standard is very helpful for musicians to tune their instruments to the same pitch. It's also helpful for building a tuner because we can use it as a reference to calculate the difference between the pitch we captured and the reference pitch.\n\n### Mapping Pitch to Note\n\nWith the pitch standard, each note will have a specific frequency. Here are the notes for a standard six-string guitar tuning and their frequencies:\n\n| Note | Frequency (Hz) |\n| ---- | -------------- |\n| E2   | 82.41          |\n| A2   | 110.00         |\n| D3   | 146.83         |\n| G3   | 196.00         |\n| B3   | 246.94         |\n| E4   | 329.63         |\n\nSo, your guitar should be tuned to E2, A2, D3, G3, B3, E4, from the lowest to the highest string.\n\n<Image\n  className=\"mx-auto\"\n  src=\"/images/tuner/tuning.png\"\n  width={278}\n  height={371}\n  alt=\"Guitar tuning\"\n/>\n\nNow, for tuning, we want to calculate the difference between the captured pitch and the reference pitch. We can then map the difference to the closest note:\n\n```tsx {14-21, 40-48}\nconst context = new AudioContext();\nconst stream = await navigator.mediaDevices.getUserMedia({ audio: true });\nconst microphone = context.createMediaStreamSource(stream);\n\nconst analyser = context.createAnalyser();\nanalyser.fftSize = 2048;\nmicrophone.connect(analyser);\n\nconst dataArray = new Float32Array(analyser.frequencyBinCount);\n\nconst minDecibels = -60; // Minimum decibels representing 0% volume\nconst maxDecibels = 0; // Maximum decibels representing 100% volume\n\nconst tuning = {\n  '82.41': 'E2',\n  '110': 'A2',\n  '146.83': 'D3',\n  '196': 'G3',\n  '246.94': 'B3',\n  '329.63': 'E4',\n} as const;\n\nconst getData = () => {\n  analyser.getFloatFrequencyData(dataArray);\n\n  const data = dataArray.reduce(\n    (acc, decibels, index) => {\n      const volume =\n        ((decibels - minDecibels) / (maxDecibels - minDecibels)) * 100;\n\n      if (volume > acc.maxVolume) {\n        return { maxIndex: index, maxVolume: volume };\n      }\n\n      return acc;\n    },\n    { maxIndex: -1, maxVolume: 0 },\n  );\n  const frequency = (data.maxIndex * context.sampleRate) / analyser.fftSize;\n  const closestPitch = Object.keys(tuning).reduce((prev, current) =>\n    Math.abs(Number(current) - frequency) < Math.abs(Number(prev) - frequency)\n      ? current\n      : prev,\n  );\n  const note = tuning[closestPitch];\n  const difference = frequency - Number(closestPitch);\n\n  // Do something with `note` and `difference` ...\n\n  requestAnimationFrame(getData);\n};\n\ngetData();\n```\n\n## Tone.js\n\nSo far, we have been using the Web Audio API directly, but there's a library called [Tone.js](https://tonejs.github.io/) which can make working with the Web Audio API much easier. It's meant for creating interactive music in the browser and provides great abstractions and utilities.\n\nHere's how we access the microphone and the analyser node using Tone.js:\n\n```tsx\nimport { Analyser, UserMedia, context, start } from 'tone';\n\nawait start();\nconst analyser = new Analyser();\nconst microphone = await new UserMedia().connect(analyser).open();\n\nconst getData = () => {\n  const dataArray = analyser.getValue();\n\n  // Do something with `dataArray` ...\n\n  requestAnimationFrame(getData);\n};\n```\n\nWay cleaner, right?\n\n## Tone.js and Pitch Detection with `pitchy`\n\nI have struggled quite a bit to get pitch detection working with [Tone.js](https://github.com/Tonejs/Tone.js) and using FFT, because for some reason it didn't provide accurate enough pitches. If the `fftSize` was set to its maximum of `32768` then every frequency was just `-Infinity`, which means no signal, no matter what.\n\nSo I looked around for alternatives on how to get it working accurately, and then stumbled upon a library called [pitchy](https://github.com/ianprime0509/pitchy) that allows me to detect the pitch provided by waveform data.\n\nHere's how this looks:\n\n```tsx\nimport { PitchDetector } from 'pitchy';\nimport { Analyser, UserMedia, context, start } from 'tone';\n\nawait start();\nconst analyser = new Analyser('waveform', 2048);\nconst microphone = await new UserMedia().connect(analyser).open();\n\nsetDevice(deviceName);\n\nconst getPitch = () => {\n  const dataArray = analyser.getValue() as Float32Array;\n  const frequency = PitchDetector.forFloat32Array(analyser.size);\n  const [pitch, clarity] = frequency.findPitch(dataArray, context.sampleRate);\n  const volumeDb = meter.getValue() as number;\n  const inputVolume = Math.min(\n    100,\n    Math.max(0, ((volumeDb - minVolume) / (maxVolume - minVolume)) * 100),\n  );\n\n  const isCaptureRange = pitch !== 0 && clarity > 0.96 && inputVolume !== 0;\n\n  if (isCaptureRange) {\n    // Do something with `pitch` ...\n  }\n\n  requestAnimationFrame(getPitch);\n};\n\ngetPitch();\n```\n\n### Comparing Waveform and FFT\n\nThe waveform data is a time-domain representation of the audio signal, while the FFT data is a frequency-domain representation of the audio signal. Personally, I experienced more accurate pitches when using waveform data and `pitchy` for pitch detection. I'd love to hear from you if you have different experiences.\n\n## Putting It All Together\n\nNow that we have the pitch captured, we can display this, among other useful data, in a React component:\n\n```tsx title=\"tuner.tsx\"\n'use client';\n\nimport type { Mode, Note } from '@/lib/modes';\nimport type { FC } from 'react';\n\nimport { modes } from '@/lib/modes';\nimport { getClosestNote, hzToCents, range } from '@/lib/pitch';\nimport { cn } from '@/lib/utils';\nimport { Mic, MicOff, Volume2 } from 'lucide-react';\nimport { PitchDetector } from 'pitchy';\nimport { useCallback, useEffect, useRef, useState } from 'react';\nimport { Analyser, Meter, UserMedia, context, start } from 'tone';\n\nimport { Button } from './ui/button';\nimport { Progress } from './ui/progress';\nimport {\n  Select,\n  SelectContent,\n  SelectGroup,\n  SelectItem,\n  SelectLabel,\n  SelectSeparator,\n  SelectTrigger,\n  SelectValue,\n} from './ui/select';\n\n// Volume ranges in dB\nconst minVolume = -48;\nconst maxVolume = -12;\n// Amount of Hz considered to be in tune\nconst deviation = 1;\nconst bars = Array.from({ length: range / 2 + 1 }, (_, i) => i - range / 4);\n\nexport const Tuner: FC = () => {\n  const requestId = useRef<number>();\n  const [mode, setMode] = useState<Mode>('chromatic');\n  const [pitch, setPitch] = useState<null | number>(null);\n  const [note, setNote] = useState<Note | null>(null);\n  const [isListening, setIsListening] = useState(false);\n  const [isCapturing, setIsCapturing] = useState(false);\n  const [volume, setVolume] = useState(0);\n  const [isInTune, setIsInTune] = useState(false);\n  const [device, setDevice] = useState<string | undefined>();\n  const tuning = modes[mode];\n  const detectPitch = useCallback(async () => {\n    await start();\n    const analyser = new Analyser('waveform', 2048);\n    const meter = new Meter();\n    const microphone = await new UserMedia()\n      .connect(analyser)\n      .connect(meter)\n      .open();\n    const devices = await navigator.mediaDevices.enumerateDevices();\n    const deviceName = devices.find(\n      (device) => device.deviceId === microphone.deviceId,\n    )?.label;\n\n    setDevice(deviceName);\n\n    const getPitch = () => {\n      const dataArray = analyser.getValue() as Float32Array;\n      const frequency = PitchDetector.forFloat32Array(analyser.size);\n      const [pitch, clarity] = frequency.findPitch(\n        dataArray,\n        context.sampleRate,\n      );\n      const volumeDb = meter.getValue() as number;\n      const inputVolume = Math.min(\n        100,\n        Math.max(0, ((volumeDb - minVolume) / (maxVolume - minVolume)) * 100),\n      );\n\n      setVolume(inputVolume);\n\n      const isCaptureRange = pitch !== 0 && clarity > 0.96 && inputVolume !== 0;\n\n      if (isCaptureRange) {\n        const closestNote = getClosestNote(pitch, mode);\n\n        if (closestNote !== null) {\n          const referencePitch = tuning[closestNote];\n          const difference = referencePitch\n            ? Math.abs(referencePitch - pitch)\n            : 0;\n          const isInTune = difference <= deviation;\n\n          setIsInTune(isInTune);\n        }\n\n        setNote(closestNote);\n        setPitch(pitch);\n      } else if (pitch) {\n        setPitch(0);\n      }\n\n      setIsCapturing(isCaptureRange);\n      requestId.current = requestAnimationFrame(getPitch);\n    };\n\n    getPitch();\n  }, [mode, tuning]);\n\n  useEffect(() => {\n    if (isListening) {\n      (() => detectPitch())();\n    }\n\n    return () => {\n      if (requestId.current) {\n        cancelAnimationFrame(requestId.current);\n      }\n    };\n  }, [detectPitch, isListening]);\n\n  const handleStart = () => {\n    setIsListening(true);\n  };\n  const handleValueChange = (value: string) => {\n    setMode(value);\n\n    if (requestId.current) {\n      cancelAnimationFrame(requestId.current);\n    }\n  };\n  const tuningPitch = note ? tuning[note] : 0;\n  const cents = pitch && tuningPitch ? hzToCents(pitch, tuningPitch) : 0;\n\n  return (\n    <div>\n      <div\n        className={cn('grid items-center gap-12', {\n          'opacity-25': !isListening,\n        })}\n      >\n        <div className=\"grid gap-12\">\n          <p\n            className={cn(\n              'text-center text-8xl font-medium',\n              isInTune ? 'text-success' : 'text-muted-foreground',\n              {\n                'opacity-0': !isCapturing || !note,\n              },\n            )}\n          >\n            {note ?? '-'}\n          </p>\n          <div className=\"grid gap-4\">\n            <div className=\"text-muted flex justify-between\">\n              <div className=\"flex-1 text-start\">-50</div>\n              <div className=\"text-center\">0</div>\n              <div className=\"flex-1 text-end\">+50</div>\n            </div>\n            <div>\n              <div\n                className={cn('flex items-end justify-between transition', {\n                  'opacity-50 delay-500': !isCapturing || !note,\n                })}\n              >\n                {bars.map((bar) => (\n                  <div key={bar}>\n                    <div\n                      className={cn(\n                        'h-8 w-px rounded sm:w-0.5',\n                        bar === 0\n                          ? 'bg-primary h-12'\n                          : bar % 5 === 0\n                            ? 'h-10 bg-gray-500'\n                            : 'bg-gray-300',\n                      )}\n                    />\n                  </div>\n                ))}\n              </div>\n              <div\n                className=\"absolute inset-x-0 top-1/2 flex -translate-y-1/2 items-center justify-center transition duration-75\"\n                style={{ transform: `translate(${cents}%, -50%)` }}\n              >\n                <div\n                  className={cn(\n                    'shadow-secondary/25 h-20 w-1 shadow-2xl transition-all',\n                    isInTune ? 'bg-success' : 'bg-muted-foreground',\n                    !isCapturing || !note\n                      ? 'scale-95 opacity-0 delay-500'\n                      : 'opacity-95',\n                  )}\n                />\n              </div>\n            </div>\n          </div>\n        </div>\n        <div className=\"grid gap-2\">\n          <p\n            className={cn(\n              'text-primary text-center text-5xl tabular-nums transition',\n              {\n                'opacity-50 delay-500 duration-400': !isCapturing || !note,\n              },\n            )}\n          >\n            {Math.round(cents)} ct\n          </p>\n\n          <p\n            className={cn('text-primary text-center tabular-nums transition', {\n              'opacity-50 delay-500 duration-400': !isCapturing || !note,\n            })}\n          >\n            {(pitch ?? 0).toFixed(1)} Hz\n          </p>\n        </div>\n        <div className=\"grid gap-4\">\n          <div className=\"flex justify-between gap-2\">\n            <Select\n              disabled={!isListening}\n              onValueChange={handleValueChange}\n              value={mode}\n            >\n              <SelectTrigger className=\"w-40\">\n                <SelectValue placeholder=\"Mode\" />\n              </SelectTrigger>\n              <SelectContent>\n                <SelectGroup>\n                  <SelectItem value=\"chromatic\">Chromatic</SelectItem>\n                </SelectGroup>\n                <SelectSeparator />\n                <SelectGroup>\n                  <SelectLabel>Guitar</SelectLabel>\n                  <SelectItem value=\"guitarStandard\">Standard</SelectItem>\n                  <SelectItem value=\"guitarDropD\">Drop D</SelectItem>\n                </SelectGroup>\n                <SelectSeparator />\n                <SelectGroup>\n                  <SelectLabel>Ukulele</SelectLabel>\n                  <SelectItem value=\"ukuleleStandard\">Standard</SelectItem>\n                </SelectGroup>\n              </SelectContent>\n            </Select>\n            <div className=\"flex items-center gap-2\">\n              {device ? (\n                <Mic className=\"text-muted-foreground h-5 w-5\" />\n              ) : (\n                <MicOff className=\"text-muted-foreground h-5 w-5\" />\n              )}\n              <p className=\"text-muted-foreground truncate\">{device}</p>\n            </div>\n          </div>\n          <div className=\"flex items-center gap-2\">\n            <Volume2 className=\"text-muted-foreground h-5 w-5\" />\n            <Progress value={volume} />\n          </div>\n        </div>\n      </div>\n      <Button\n        className={cn(\n          'absolute top-1/2 left-1/2 -translate-x-1/2 -translate-y-1/2 transition duration-400',\n          { 'pointer-events-none opacity-0 blur-2xl': isListening },\n        )}\n        onClick={handleStart}\n        size=\"lg\"\n      >\n        Start Tuning\n      </Button>\n    </div>\n  );\n};\n```\n\nAnd here's how the UI looks like:\n\n<Image src=\"/images/tuner/tuner.png\" width={2628} height={1816} alt=\"Tuner\" />\n\n## Final Thoughts\n\nBuilding a tuner was a really interesting learning experience in getting into the Web Audio API. I think for beginners to the Web Audio API, building a tuner might be more ambitious than just simply working with an oscillator or with samples first to create sound. With a tuner, I had to play a lot with pitches and frequencies, and it involves a bit of math to get it right.\n\nNevertheless, I had fun working with [Tone.js](https://github.com/Tonejs/Tone.js) and getting the correct pitch using the `pitchy` library. It's also interesting to see a tuner from a technical perspective because I had never thought about how it works under the hood.\n\nThe entire code is available in the [GitHub repository](https://github.com/glennreyes/tuner). I hope you find this helpful and inspiring to build your own tuner or any other music-related application.",
    "description": "I had no idea how the Web Audio API worked, so I decided to build a tuner with it. Here's what I learned.",
    "publishedAt": "2024-02-23",
    "title": "Building a Tuner with Tone.js and React",
    "_meta": {
      "filePath": "tuner.mdx",
      "fileName": "tuner.mdx",
      "directory": ".",
      "extension": "mdx",
      "path": "tuner"
    },
    "body": "Having attended many tech events over the past years, I have always admired people who showed off their musical skills on stage. Few examples:\n\n- [Beats in the Browser](https://youtu.be/z7EQtSK3QHM?feature=shared) by [Ken Wheeler](https://twitter.com/ken_wheeler)\n- [Alive and Kicking. A Vue into Rock& Roll!](https://youtu.be/-4m4TIJ0z20?feature=shared) by [Tim Benniks](https://twitter.com/timbenniks)\n- [Beats in the Browser - Coding Music with JavaScript](https://youtu.be/KtVILY90t4g?feature=shared) by [Rowdy Rabouw](https://twitter.com/RowdyRabouw)\n\nAs someone enthusiastic about playing guitar and making music, I have dreamed of building a fully-fledged web-based DAW (Digital Audio Workstation), but I had never looked into the Web Audio API to see what it's capable of. So, I took some time and thought, why not start with something small and build a tuner for musical instruments?\n\n## Web Audio API\n\nThe Web Audio API is a powerful JavaScript API that allows developers to do a lot of things, but most notably: creating and manipulating audio in the browser. It is a very powerful API for creating music, sound effects, and other audio applications.\n\nBefore I had any code written, I knew I would need to get a microphone input and then work from there:\n\n```tsx\nconst context = new AudioContext();\nconst stream = await navigator.mediaDevices.getUserMedia({ audio: true });\nconst microphone = context.createMediaStreamSource(stream);\n```\n\n1. `const context = new AudioContext();` - Represents an audio-processing graph built from audio modules linked together. The `AudioContext` is the main entry point to the Web Audio API and is used to create and manage audio nodes, handle audio processing, and control audio playback.\n2. `const stream = await navigator.mediaDevices.getUserMedia({ audio: true });` - Prompts the user for permission to use a media input that produces audio (such as a microphone). It returns a `Promise` of a `MediaStream` representing the user's audio input.\n3. `const microphone = context.createMediaStreamSource(stream);` - Creates an `AudioNode` from the user's audio stream obtained in the previous step. This `AudioNode` can then be connected to other nodes in the `AudioContext` for processing, analysis, or output.\n\nNow, with `microphone`, we can't really do anything helpful yet, so we need to add an `AnalyserNode` to the `AudioContext` to get the frequency data from the microphone input.\n\n<Image\n  className=\"bg-white\"\n  src=\"/images/tuner/analyser-node.svg\"\n  width={752}\n  height={224}\n  alt=\"AnalyserNode\"\n/>\n\nThat's how it looks like in code:\n\n```tsx {5-9}\nconst context = new AudioContext();\nconst stream = await navigator.mediaDevices.getUserMedia({ audio: true });\nconst microphone = context.createMediaStreamSource(stream);\n\nconst analyser = context.createAnalyser();\nmicrophone.connect(analyser);\n\nconst dataArray = new Float32Array(analyser.frequencyBinCount);\nanalyser.getFloatFrequencyData(dataArray);\n```\n\n1. We create an `AnalyserNode` using `context.createAnalyser()`.\n2. We connect the `microphone` to the `analyser` using `microphone.connect(analyser)`.\n3. We create a `Float32Array` to hold the frequency data represented as FFT (Fast Fourier Transform) data.\n4. We call `analyser.getFloatFrequencyData(dataArray)` to copy the current frequency data into `dataArray`, where each value is a sample represented as a decibel value for a particular frequency.\n\n<Image\n  className=\"mx-auto\"\n  src=\"/images/tuner/data-array.png\"\n  width={642}\n  height={384}\n  alt=\"Example of dataArray values\"\n/>\n\nSometimes it can contain `-Infinity`, which means that the sample is silent.\n\nNow that we have a \"snapshot\" of frequency data, we want to capture frequency data over time by using `requestAnimationFrame`:\n\n```tsx {10-16}\nconst context = new AudioContext();\nconst stream = await navigator.mediaDevices.getUserMedia({ audio: true });\nconst microphone = context.createMediaStreamSource(stream);\n\nconst analyser = context.createAnalyser();\nanalyser.fftSize = 2048;\nmicrophone.connect(analyser);\n\nconst dataArray = new Float32Array(analyser.frequencyBinCount);\nconst getData = () => {\n  analyser.getFloatFrequencyData(dataArray);\n\n  requestAnimationFrame(getData);\n};\n\ngetData();\n```\n\n## Capturing the Pitch\n\nOnce we have that, we want to capture the most dominant sample from the frequency data. A perfect job for looping through `dataArray` with `.reduce` and capturing the one with the highest value.\n\nSince these are represented in decibels, we specify a range of what we want to capture from, the minimum and maximum volume.\n\nFinally, we capture the most dominant sample and convert it to the actually frequency (or pitch), as each index of `dataArray` represents a frequency of half the sample rate. So, we can calculate the frequency by `(index * context.sampleRate) / analyser.fftSize`.\n\nHere's how it looks:\n\n```tsx {11-12, 17-30}\nconst context = new AudioContext();\nconst stream = await navigator.mediaDevices.getUserMedia({ audio: true });\nconst microphone = context.createMediaStreamSource(stream);\n\nconst analyser = context.createAnalyser();\nanalyser.fftSize = 2048;\nmicrophone.connect(analyser);\n\nconst dataArray = new Float32Array(analyser.frequencyBinCount);\n\nconst minDecibels = -60; // Minimum decibels representing 0% volume\nconst maxDecibels = 0; // Maximum decibels representing 100% volume\n\nconst getData = () => {\n  analyser.getFloatFrequencyData(dataArray);\n\n  const data = dataArray.reduce(\n    (acc, decibels, index) => {\n      const volume =\n        ((decibels - minDecibels) / (maxDecibels - minDecibels)) * 100;\n\n      if (volume > acc.maxVolume) {\n        return { maxIndex: index, maxVolume: volume };\n      }\n\n      return acc;\n    },\n    { maxIndex: -1, maxVolume: 0 },\n  );\n  const frequency = (data.maxIndex * context.sampleRate) / analyser.fftSize;\n\n  // Do something with `frequency` ...\n\n  requestAnimationFrame(getData);\n};\n\ngetData();\n```\n\n## From Pitch to Note\n\nMapping a pitch to note is fairly simple. Before I go more into that, I think it's important to know about the reference pitch of 440 Hertz, also known as standard pitch, concert pitch or A440.\n\n### The Pitch Standard\n\nThe pitch standard, also known as A440, is a very common musical pitch standard that specifies the frequency of the A above middle C on the piano as 440 Hz:\n\n<Image\n  className=\"bg-white p-4\"\n  src=\"/images/tuner/a440.svg\"\n  width={752}\n  height={171}\n  alt=\"AnalyserNode\"\n/>\n\nHaving this standard is very helpful for musicians to tune their instruments to the same pitch. It's also helpful for building a tuner because we can use it as a reference to calculate the difference between the pitch we captured and the reference pitch.\n\n### Mapping Pitch to Note\n\nWith the pitch standard, each note will have a specific frequency. Here are the notes for a standard six-string guitar tuning and their frequencies:\n\n| Note | Frequency (Hz) |\n| ---- | -------------- |\n| E2   | 82.41          |\n| A2   | 110.00         |\n| D3   | 146.83         |\n| G3   | 196.00         |\n| B3   | 246.94         |\n| E4   | 329.63         |\n\nSo, your guitar should be tuned to E2, A2, D3, G3, B3, E4, from the lowest to the highest string.\n\n<Image\n  className=\"mx-auto\"\n  src=\"/images/tuner/tuning.png\"\n  width={278}\n  height={371}\n  alt=\"Guitar tuning\"\n/>\n\nNow, for tuning, we want to calculate the difference between the captured pitch and the reference pitch. We can then map the difference to the closest note:\n\n```tsx {14-21, 40-48}\nconst context = new AudioContext();\nconst stream = await navigator.mediaDevices.getUserMedia({ audio: true });\nconst microphone = context.createMediaStreamSource(stream);\n\nconst analyser = context.createAnalyser();\nanalyser.fftSize = 2048;\nmicrophone.connect(analyser);\n\nconst dataArray = new Float32Array(analyser.frequencyBinCount);\n\nconst minDecibels = -60; // Minimum decibels representing 0% volume\nconst maxDecibels = 0; // Maximum decibels representing 100% volume\n\nconst tuning = {\n  '82.41': 'E2',\n  '110': 'A2',\n  '146.83': 'D3',\n  '196': 'G3',\n  '246.94': 'B3',\n  '329.63': 'E4',\n} as const;\n\nconst getData = () => {\n  analyser.getFloatFrequencyData(dataArray);\n\n  const data = dataArray.reduce(\n    (acc, decibels, index) => {\n      const volume =\n        ((decibels - minDecibels) / (maxDecibels - minDecibels)) * 100;\n\n      if (volume > acc.maxVolume) {\n        return { maxIndex: index, maxVolume: volume };\n      }\n\n      return acc;\n    },\n    { maxIndex: -1, maxVolume: 0 },\n  );\n  const frequency = (data.maxIndex * context.sampleRate) / analyser.fftSize;\n  const closestPitch = Object.keys(tuning).reduce((prev, current) =>\n    Math.abs(Number(current) - frequency) < Math.abs(Number(prev) - frequency)\n      ? current\n      : prev,\n  );\n  const note = tuning[closestPitch];\n  const difference = frequency - Number(closestPitch);\n\n  // Do something with `note` and `difference` ...\n\n  requestAnimationFrame(getData);\n};\n\ngetData();\n```\n\n## Tone.js\n\nSo far, we have been using the Web Audio API directly, but there's a library called [Tone.js](https://tonejs.github.io/) which can make working with the Web Audio API much easier. It's meant for creating interactive music in the browser and provides great abstractions and utilities.\n\nHere's how we access the microphone and the analyser node using Tone.js:\n\n```tsx\nimport { Analyser, UserMedia, context, start } from 'tone';\n\nawait start();\nconst analyser = new Analyser();\nconst microphone = await new UserMedia().connect(analyser).open();\n\nconst getData = () => {\n  const dataArray = analyser.getValue();\n\n  // Do something with `dataArray` ...\n\n  requestAnimationFrame(getData);\n};\n```\n\nWay cleaner, right?\n\n## Tone.js and Pitch Detection with `pitchy`\n\nI have struggled quite a bit to get pitch detection working with [Tone.js](https://github.com/Tonejs/Tone.js) and using FFT, because for some reason it didn't provide accurate enough pitches. If the `fftSize` was set to its maximum of `32768` then every frequency was just `-Infinity`, which means no signal, no matter what.\n\nSo I looked around for alternatives on how to get it working accurately, and then stumbled upon a library called [pitchy](https://github.com/ianprime0509/pitchy) that allows me to detect the pitch provided by waveform data.\n\nHere's how this looks:\n\n```tsx\nimport { PitchDetector } from 'pitchy';\nimport { Analyser, UserMedia, context, start } from 'tone';\n\nawait start();\nconst analyser = new Analyser('waveform', 2048);\nconst microphone = await new UserMedia().connect(analyser).open();\n\nsetDevice(deviceName);\n\nconst getPitch = () => {\n  const dataArray = analyser.getValue() as Float32Array;\n  const frequency = PitchDetector.forFloat32Array(analyser.size);\n  const [pitch, clarity] = frequency.findPitch(dataArray, context.sampleRate);\n  const volumeDb = meter.getValue() as number;\n  const inputVolume = Math.min(\n    100,\n    Math.max(0, ((volumeDb - minVolume) / (maxVolume - minVolume)) * 100),\n  );\n\n  const isCaptureRange = pitch !== 0 && clarity > 0.96 && inputVolume !== 0;\n\n  if (isCaptureRange) {\n    // Do something with `pitch` ...\n  }\n\n  requestAnimationFrame(getPitch);\n};\n\ngetPitch();\n```\n\n### Comparing Waveform and FFT\n\nThe waveform data is a time-domain representation of the audio signal, while the FFT data is a frequency-domain representation of the audio signal. Personally, I experienced more accurate pitches when using waveform data and `pitchy` for pitch detection. I'd love to hear from you if you have different experiences.\n\n## Putting It All Together\n\nNow that we have the pitch captured, we can display this, among other useful data, in a React component:\n\n```tsx title=\"tuner.tsx\"\n'use client';\n\nimport type { Mode, Note } from '@/lib/modes';\nimport type { FC } from 'react';\n\nimport { modes } from '@/lib/modes';\nimport { getClosestNote, hzToCents, range } from '@/lib/pitch';\nimport { cn } from '@/lib/utils';\nimport { Mic, MicOff, Volume2 } from 'lucide-react';\nimport { PitchDetector } from 'pitchy';\nimport { useCallback, useEffect, useRef, useState } from 'react';\nimport { Analyser, Meter, UserMedia, context, start } from 'tone';\n\nimport { Button } from './ui/button';\nimport { Progress } from './ui/progress';\nimport {\n  Select,\n  SelectContent,\n  SelectGroup,\n  SelectItem,\n  SelectLabel,\n  SelectSeparator,\n  SelectTrigger,\n  SelectValue,\n} from './ui/select';\n\n// Volume ranges in dB\nconst minVolume = -48;\nconst maxVolume = -12;\n// Amount of Hz considered to be in tune\nconst deviation = 1;\nconst bars = Array.from({ length: range / 2 + 1 }, (_, i) => i - range / 4);\n\nexport const Tuner: FC = () => {\n  const requestId = useRef<number>();\n  const [mode, setMode] = useState<Mode>('chromatic');\n  const [pitch, setPitch] = useState<null | number>(null);\n  const [note, setNote] = useState<Note | null>(null);\n  const [isListening, setIsListening] = useState(false);\n  const [isCapturing, setIsCapturing] = useState(false);\n  const [volume, setVolume] = useState(0);\n  const [isInTune, setIsInTune] = useState(false);\n  const [device, setDevice] = useState<string | undefined>();\n  const tuning = modes[mode];\n  const detectPitch = useCallback(async () => {\n    await start();\n    const analyser = new Analyser('waveform', 2048);\n    const meter = new Meter();\n    const microphone = await new UserMedia()\n      .connect(analyser)\n      .connect(meter)\n      .open();\n    const devices = await navigator.mediaDevices.enumerateDevices();\n    const deviceName = devices.find(\n      (device) => device.deviceId === microphone.deviceId,\n    )?.label;\n\n    setDevice(deviceName);\n\n    const getPitch = () => {\n      const dataArray = analyser.getValue() as Float32Array;\n      const frequency = PitchDetector.forFloat32Array(analyser.size);\n      const [pitch, clarity] = frequency.findPitch(\n        dataArray,\n        context.sampleRate,\n      );\n      const volumeDb = meter.getValue() as number;\n      const inputVolume = Math.min(\n        100,\n        Math.max(0, ((volumeDb - minVolume) / (maxVolume - minVolume)) * 100),\n      );\n\n      setVolume(inputVolume);\n\n      const isCaptureRange = pitch !== 0 && clarity > 0.96 && inputVolume !== 0;\n\n      if (isCaptureRange) {\n        const closestNote = getClosestNote(pitch, mode);\n\n        if (closestNote !== null) {\n          const referencePitch = tuning[closestNote];\n          const difference = referencePitch\n            ? Math.abs(referencePitch - pitch)\n            : 0;\n          const isInTune = difference <= deviation;\n\n          setIsInTune(isInTune);\n        }\n\n        setNote(closestNote);\n        setPitch(pitch);\n      } else if (pitch) {\n        setPitch(0);\n      }\n\n      setIsCapturing(isCaptureRange);\n      requestId.current = requestAnimationFrame(getPitch);\n    };\n\n    getPitch();\n  }, [mode, tuning]);\n\n  useEffect(() => {\n    if (isListening) {\n      (() => detectPitch())();\n    }\n\n    return () => {\n      if (requestId.current) {\n        cancelAnimationFrame(requestId.current);\n      }\n    };\n  }, [detectPitch, isListening]);\n\n  const handleStart = () => {\n    setIsListening(true);\n  };\n  const handleValueChange = (value: string) => {\n    setMode(value);\n\n    if (requestId.current) {\n      cancelAnimationFrame(requestId.current);\n    }\n  };\n  const tuningPitch = note ? tuning[note] : 0;\n  const cents = pitch && tuningPitch ? hzToCents(pitch, tuningPitch) : 0;\n\n  return (\n    <div>\n      <div\n        className={cn('grid items-center gap-12', {\n          'opacity-25': !isListening,\n        })}\n      >\n        <div className=\"grid gap-12\">\n          <p\n            className={cn(\n              'text-center text-8xl font-medium',\n              isInTune ? 'text-success' : 'text-muted-foreground',\n              {\n                'opacity-0': !isCapturing || !note,\n              },\n            )}\n          >\n            {note ?? '-'}\n          </p>\n          <div className=\"grid gap-4\">\n            <div className=\"text-muted flex justify-between\">\n              <div className=\"flex-1 text-start\">-50</div>\n              <div className=\"text-center\">0</div>\n              <div className=\"flex-1 text-end\">+50</div>\n            </div>\n            <div>\n              <div\n                className={cn('flex items-end justify-between transition', {\n                  'opacity-50 delay-500': !isCapturing || !note,\n                })}\n              >\n                {bars.map((bar) => (\n                  <div key={bar}>\n                    <div\n                      className={cn(\n                        'h-8 w-px rounded sm:w-0.5',\n                        bar === 0\n                          ? 'bg-primary h-12'\n                          : bar % 5 === 0\n                            ? 'h-10 bg-gray-500'\n                            : 'bg-gray-300',\n                      )}\n                    />\n                  </div>\n                ))}\n              </div>\n              <div\n                className=\"absolute inset-x-0 top-1/2 flex -translate-y-1/2 items-center justify-center transition duration-75\"\n                style={{ transform: `translate(${cents}%, -50%)` }}\n              >\n                <div\n                  className={cn(\n                    'shadow-secondary/25 h-20 w-1 shadow-2xl transition-all',\n                    isInTune ? 'bg-success' : 'bg-muted-foreground',\n                    !isCapturing || !note\n                      ? 'scale-95 opacity-0 delay-500'\n                      : 'opacity-95',\n                  )}\n                />\n              </div>\n            </div>\n          </div>\n        </div>\n        <div className=\"grid gap-2\">\n          <p\n            className={cn(\n              'text-primary text-center text-5xl tabular-nums transition',\n              {\n                'opacity-50 delay-500 duration-400': !isCapturing || !note,\n              },\n            )}\n          >\n            {Math.round(cents)} ct\n          </p>\n\n          <p\n            className={cn('text-primary text-center tabular-nums transition', {\n              'opacity-50 delay-500 duration-400': !isCapturing || !note,\n            })}\n          >\n            {(pitch ?? 0).toFixed(1)} Hz\n          </p>\n        </div>\n        <div className=\"grid gap-4\">\n          <div className=\"flex justify-between gap-2\">\n            <Select\n              disabled={!isListening}\n              onValueChange={handleValueChange}\n              value={mode}\n            >\n              <SelectTrigger className=\"w-40\">\n                <SelectValue placeholder=\"Mode\" />\n              </SelectTrigger>\n              <SelectContent>\n                <SelectGroup>\n                  <SelectItem value=\"chromatic\">Chromatic</SelectItem>\n                </SelectGroup>\n                <SelectSeparator />\n                <SelectGroup>\n                  <SelectLabel>Guitar</SelectLabel>\n                  <SelectItem value=\"guitarStandard\">Standard</SelectItem>\n                  <SelectItem value=\"guitarDropD\">Drop D</SelectItem>\n                </SelectGroup>\n                <SelectSeparator />\n                <SelectGroup>\n                  <SelectLabel>Ukulele</SelectLabel>\n                  <SelectItem value=\"ukuleleStandard\">Standard</SelectItem>\n                </SelectGroup>\n              </SelectContent>\n            </Select>\n            <div className=\"flex items-center gap-2\">\n              {device ? (\n                <Mic className=\"text-muted-foreground h-5 w-5\" />\n              ) : (\n                <MicOff className=\"text-muted-foreground h-5 w-5\" />\n              )}\n              <p className=\"text-muted-foreground truncate\">{device}</p>\n            </div>\n          </div>\n          <div className=\"flex items-center gap-2\">\n            <Volume2 className=\"text-muted-foreground h-5 w-5\" />\n            <Progress value={volume} />\n          </div>\n        </div>\n      </div>\n      <Button\n        className={cn(\n          'absolute top-1/2 left-1/2 -translate-x-1/2 -translate-y-1/2 transition duration-400',\n          { 'pointer-events-none opacity-0 blur-2xl': isListening },\n        )}\n        onClick={handleStart}\n        size=\"lg\"\n      >\n        Start Tuning\n      </Button>\n    </div>\n  );\n};\n```\n\nAnd here's how the UI looks like:\n\n<Image src=\"/images/tuner/tuner.png\" width={2628} height={1816} alt=\"Tuner\" />\n\n## Final Thoughts\n\nBuilding a tuner was a really interesting learning experience in getting into the Web Audio API. I think for beginners to the Web Audio API, building a tuner might be more ambitious than just simply working with an oscillator or with samples first to create sound. With a tuner, I had to play a lot with pitches and frequencies, and it involves a bit of math to get it right.\n\nNevertheless, I had fun working with [Tone.js](https://github.com/Tonejs/Tone.js) and getting the correct pitch using the `pitchy` library. It's also interesting to see a tuner from a technical perspective because I had never thought about how it works under the hood.\n\nThe entire code is available in the [GitHub repository](https://github.com/glennreyes/tuner). I hope you find this helpful and inspiring to build your own tuner or any other music-related application.",
    "slug": "tuner"
  }
]